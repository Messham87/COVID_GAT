{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Run_GAN_Colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0LysuYDYALe"
      },
      "source": [
        "# !mkdir /content/drive/MyDrive/COVID_GAT/"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCF5MSE9DDVU"
      },
      "source": [
        "# !git clone https://github.com/Messham87/COVID_GAT.git ."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YgdlCSYDEEl"
      },
      "source": [
        "# !git init"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhRoZIBT5QtL"
      },
      "source": [
        "# !git config --global user.email \"Messham87@gmail.com\"\n",
        "# !git config --global user.name \"Stephen Messham\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU6yahzHv7iy"
      },
      "source": [
        "## Set Up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEuB_g1Wdvf3",
        "outputId": "4f496a3a-77ca-42f7-af14-96555b96eaa7"
      },
      "source": [
        "%cd /content/drive/MyDrive/COVID_GAT"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/COVID_GAT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlf5gjHxSPxz"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import mean_squared_log_error, mean_absolute_error, mean_squared_error\n",
        "from scipy import stats\n",
        "from utils import load_data\n",
        "from models import GATMLP, OneLayerGAT, TwoLayerGAT, ThreeLayerGAT, TwoLayerGATMLP, ThreeLayerGATMLP, MLP, GCN"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0TIl1XJD5FE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea6a323-7582-407c-d7fc-9e44641ca2f0"
      },
      "source": [
        "!git remote set-url origin 'https://Messham87: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@github.com/Messham87/COVID_GAT.git'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPPd-ncEXW7n",
        "outputId": "816ef295-4a39-4b63-a47b-d9b9eb351811"
      },
      "source": [
        "!git pull origin"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/17)\u001b[K\rremote: Counting objects:  11% (2/17)\u001b[K\rremote: Counting objects:  17% (3/17)\u001b[K\rremote: Counting objects:  23% (4/17)\u001b[K\rremote: Counting objects:  29% (5/17)\u001b[K\rremote: Counting objects:  35% (6/17)\u001b[K\rremote: Counting objects:  41% (7/17)\u001b[K\rremote: Counting objects:  47% (8/17)\u001b[K\rremote: Counting objects:  52% (9/17)\u001b[K\rremote: Counting objects:  58% (10/17)\u001b[K\rremote: Counting objects:  64% (11/17)\u001b[K\rremote: Counting objects:  70% (12/17)\u001b[K\rremote: Counting objects:  76% (13/17)\u001b[K\rremote: Counting objects:  82% (14/17)\u001b[K\rremote: Counting objects:  88% (15/17)\u001b[K\rremote: Counting objects:  94% (16/17)\u001b[K\rremote: Counting objects: 100% (17/17)\u001b[K\rremote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects:  14% (1/7)\u001b[K\rremote: Compressing objects:  28% (2/7)\u001b[K\rremote: Compressing objects:  42% (3/7)\u001b[K\rremote: Compressing objects:  57% (4/7)\u001b[K\rremote: Compressing objects:  71% (5/7)\u001b[K\rremote: Compressing objects:  85% (6/7)\u001b[K\rremote: Compressing objects: 100% (7/7)\u001b[K\rremote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 11 (delta 4), reused 10 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (11/11), done.\n",
            "From https://github.com/Messham87/COVID_GAT\n",
            "   6ff8786..d279854  master     -> origin/master\n",
            "Updating 6ff8786..d279854\n",
            "Fast-forward\n",
            " data/covid/edge_list            | Bin \u001b[31m36501\u001b[m -> \u001b[32m110431\u001b[m bytes\n",
            " data/covid/test_idx_features_y  | Bin \u001b[31m843056\u001b[m -> \u001b[32m843056\u001b[m bytes\n",
            " data/covid/train_idx_features_y | Bin \u001b[31m843056\u001b[m -> \u001b[32m843056\u001b[m bytes\n",
            " data/covid/valid_idx_features_y | Bin \u001b[31m843056\u001b[m -> \u001b[32m843056\u001b[m bytes\n",
            " traintwolayergat.py             |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 5 files changed, 1 insertion(+), 1 deletion(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkOUKvYT5lYD",
        "outputId": "13eaac10-409e-406e-807b-5b2012ea1406"
      },
      "source": [
        "# !git commit"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hint: Waiting for your editor to close the file... error: unable to start editor 'editor'\n",
            "Please supply the message using either -m or -F option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pu9F6pzk45g6",
        "outputId": "62015dd9-0cf4-4d4b-ee4d-48e8e67f425b"
      },
      "source": [
        "# !git status"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git reset HEAD <file>...\" to unstage)\n",
            "\n",
            "\t\u001b[32mmodified:   data/covid/edge_list\u001b[m\n",
            "\t\u001b[32mnew file:   data/covid/pops1119.xlsx\u001b[m\n",
            "\t\u001b[32mmodified:   data/covid/test_idx_features_y\u001b[m\n",
            "\t\u001b[32mmodified:   data/covid/train_idx_features_y\u001b[m\n",
            "\t\u001b[32mmodified:   data/covid/valid_idx_features_y\u001b[m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwU5IsEI5V8z",
        "outputId": "a4311f49-8fcd-4dc1-8513-5bd298e940ee"
      },
      "source": [
        "# !git push -u origin master"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'master' set up to track remote branch 'master' from 'origin'.\n",
            "Everything up-to-date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9EZgSOKS59c"
      },
      "source": [
        "## Train one Layer basic GAT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqa0VKTLSoTn",
        "outputId": "09e7a0b4-e6f9-4ae1-8b08-25368aa3b5ba"
      },
      "source": [
        "# Training settings\n",
        "nocuda = False\n",
        "fastmode = False\n",
        "sparse = False\n",
        "seed = 72\n",
        "epochs = 1000\n",
        "lr = 0.005\n",
        "weight_decay = 5e-3\n",
        "hidden = 8\n",
        "nb_heads = 8\n",
        "dropout = 0.2\n",
        "alpha = 0.2\n",
        "patience = 30\n",
        "nclass = int(1)\n",
        "\n",
        "cuda = not nocuda and torch.cuda.is_available()\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Load data\n",
        "adj, train_features, train_labels, valid_features, valid_labels, test_features, test_labels = load_data()\n",
        "\n",
        "# Model and optimizer\n",
        "model = OneLayerGAT(nfeat=train_features.shape[1],\n",
        "                nhid=hidden,\n",
        "                nclass=nclass,\n",
        "                dropout=dropout,\n",
        "                nheads=nb_heads,\n",
        "                alpha=alpha)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=lr,\n",
        "                       weight_decay=weight_decay)\n",
        "print(model)\n",
        "\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    adj = adj.cuda()\n",
        "    train_features = train_features.cuda()\n",
        "    valid_features = valid_features.cuda()\n",
        "    test_features = test_features.cuda()\n",
        "    train_labels = train_labels.cuda()\n",
        "    valid_labels = valid_labels.cuda()\n",
        "    test_labels = test_labels.cuda()\n",
        "\n",
        "adj, train_features, valid_features, test_features, train_labels, valid_labels, test_labels = Variable(adj), Variable(train_features), Variable(valid_features), Variable(test_features), Variable(train_labels), Variable(valid_labels), Variable(test_labels)\n",
        "loss = nn.L1Loss()\n",
        "acc = nn.MSELoss()\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    train_output = model(train_features, adj)\n",
        "    loss_train = loss(train_output, train_labels)\n",
        "    acc_train = torch.sqrt(acc(torch.log(train_output+1), torch.log(train_labels+1)))\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        valid_output = model(valid_features, adj)\n",
        "\n",
        "        loss_val = loss(valid_output, valid_labels)\n",
        "        acc_val = torch.sqrt(acc(torch.log(valid_output+1), torch.log(valid_labels+1)))\n",
        "        print('Epoch: {:04d}'.format(epoch+1),\n",
        "              'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "              'acc_train: {:.4f}'.format(acc_train),\n",
        "              'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "              'acc_val: {:.4f}'.format(acc_val),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_val.data.item()\n",
        "\n",
        "\n",
        "def compute_test():\n",
        "    model.eval()\n",
        "    test_output = model(test_features, adj)\n",
        "    loss_test = loss(test_output, test_labels)\n",
        "    acc_test = torch.sqrt(acc(torch.log(test_output+1), torch.log(test_labels+1)))\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "loss_values = []\n",
        "bad_counter = 0\n",
        "best = epochs + 1\n",
        "best_epoch = 0\n",
        "for epoch in range(epochs):\n",
        "    loss_values.append(train(epoch))\n",
        "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "    if loss_values[-1] < best:\n",
        "        best = loss_values[-1]\n",
        "        best_epoch = epoch\n",
        "        bad_counter = 0\n",
        "    else:\n",
        "        bad_counter += 1\n",
        "\n",
        "    if bad_counter == patience:\n",
        "        break\n",
        "\n",
        "    files = glob.glob('*.pkl')\n",
        "    for file in files:\n",
        "        epoch_nb = int(file.split('.')[0])\n",
        "        if epoch_nb < best_epoch:\n",
        "            open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "            os.remove(file)\n",
        "\n",
        "files = glob.glob('*.pkl')\n",
        "for file in files:\n",
        "    epoch_nb = int(file.split('.')[0])\n",
        "    if epoch_nb > best_epoch:\n",
        "        open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "        os.remove(file)\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Restore best model;\n",
        "print('Loading epoch {}'.format(best_epoch))\n",
        "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "# Testing\n",
        "compute_test()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading covid dataset...\n",
            "OneLayerGAT(\n",
            "  (attention_0): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_1): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_2): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_3): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_4): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_5): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_6): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_7): GraphAttentionLayer (276 -> 8)\n",
            "  (out_att): GraphAttentionLayer (64 -> 64)\n",
            "  (lin1): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:97: UserWarning: Using a target size (torch.Size([379])) that is different to the input size (torch.Size([379, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([379])) that is different to the input size (torch.Size([379, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 79.5694 acc_train: 4.0173 loss_val: 74.9229 acc_val: 3.6887 time: 0.2126s\n",
            "Epoch: 0002 loss_train: 79.3930 acc_train: 3.8745 loss_val: 74.7099 acc_val: 3.5584 time: 0.0115s\n",
            "Epoch: 0003 loss_train: 79.1949 acc_train: 3.7378 loss_val: 74.4121 acc_val: 3.4019 time: 0.0110s\n",
            "Epoch: 0004 loss_train: 78.9736 acc_train: 3.6053 loss_val: 74.1245 acc_val: 3.2721 time: 0.0119s\n",
            "Epoch: 0005 loss_train: 78.7238 acc_train: 3.4753 loss_val: 73.8339 acc_val: 3.1564 time: 0.0111s\n",
            "Epoch: 0006 loss_train: 78.4593 acc_train: 3.3562 loss_val: 73.5242 acc_val: 3.0474 time: 0.0117s\n",
            "Epoch: 0007 loss_train: 78.1417 acc_train: 3.2297 loss_val: 73.1713 acc_val: 2.9368 time: 0.0111s\n",
            "Epoch: 0008 loss_train: 77.8454 acc_train: 3.1264 loss_val: 72.8336 acc_val: 2.8418 time: 0.0134s\n",
            "Epoch: 0009 loss_train: 77.5449 acc_train: 3.0310 loss_val: 72.5546 acc_val: 2.7698 time: 0.0112s\n",
            "Epoch: 0010 loss_train: 77.2493 acc_train: 2.9459 loss_val: 72.2697 acc_val: 2.7014 time: 0.0121s\n",
            "Epoch: 0011 loss_train: 76.9972 acc_train: 2.8790 loss_val: 72.0108 acc_val: 2.6435 time: 0.0110s\n",
            "Epoch: 0012 loss_train: 76.6536 acc_train: 2.7924 loss_val: 71.7464 acc_val: 2.5880 time: 0.0110s\n",
            "Epoch: 0013 loss_train: 76.4424 acc_train: 2.7453 loss_val: 71.4653 acc_val: 2.5323 time: 0.0111s\n",
            "Epoch: 0014 loss_train: 76.1561 acc_train: 2.6819 loss_val: 71.1848 acc_val: 2.4792 time: 0.0109s\n",
            "Epoch: 0015 loss_train: 75.9048 acc_train: 2.6305 loss_val: 70.8846 acc_val: 2.4255 time: 0.0116s\n",
            "Epoch: 0016 loss_train: 75.6941 acc_train: 2.5916 loss_val: 70.5640 acc_val: 2.3716 time: 0.0124s\n",
            "Epoch: 0017 loss_train: 75.4611 acc_train: 2.5492 loss_val: 70.2819 acc_val: 2.3266 time: 0.0124s\n",
            "Epoch: 0018 loss_train: 75.2423 acc_train: 2.5125 loss_val: 70.0072 acc_val: 2.2850 time: 0.0122s\n",
            "Epoch: 0019 loss_train: 74.8772 acc_train: 2.4488 loss_val: 69.7485 acc_val: 2.2476 time: 0.0117s\n",
            "Epoch: 0020 loss_train: 74.5748 acc_train: 2.3989 loss_val: 69.4997 acc_val: 2.2130 time: 0.0118s\n",
            "Epoch: 0021 loss_train: 74.3658 acc_train: 2.3690 loss_val: 69.2425 acc_val: 2.1785 time: 0.0116s\n",
            "Epoch: 0022 loss_train: 74.0741 acc_train: 2.3256 loss_val: 68.9765 acc_val: 2.1443 time: 0.0111s\n",
            "Epoch: 0023 loss_train: 73.9716 acc_train: 2.3157 loss_val: 68.7072 acc_val: 2.1109 time: 0.0108s\n",
            "Epoch: 0024 loss_train: 73.5154 acc_train: 2.2485 loss_val: 68.4347 acc_val: 2.0783 time: 0.0126s\n",
            "Epoch: 0025 loss_train: 73.4443 acc_train: 2.2444 loss_val: 68.1797 acc_val: 2.0489 time: 0.0110s\n",
            "Epoch: 0026 loss_train: 73.2030 acc_train: 2.2140 loss_val: 67.9264 acc_val: 2.0207 time: 0.0114s\n",
            "Epoch: 0027 loss_train: 72.8114 acc_train: 2.1628 loss_val: 67.6695 acc_val: 1.9930 time: 0.0126s\n",
            "Epoch: 0028 loss_train: 72.6592 acc_train: 2.1474 loss_val: 67.4135 acc_val: 1.9662 time: 0.0139s\n",
            "Epoch: 0029 loss_train: 72.3510 acc_train: 2.1111 loss_val: 67.1577 acc_val: 1.9403 time: 0.0115s\n",
            "Epoch: 0030 loss_train: 72.0784 acc_train: 2.0811 loss_val: 66.9008 acc_val: 1.9150 time: 0.0130s\n",
            "Epoch: 0031 loss_train: 71.9485 acc_train: 2.0700 loss_val: 66.6422 acc_val: 1.8903 time: 0.0125s\n",
            "Epoch: 0032 loss_train: 71.6318 acc_train: 2.0355 loss_val: 66.3722 acc_val: 1.8651 time: 0.0139s\n",
            "Epoch: 0033 loss_train: 71.3160 acc_train: 2.0028 loss_val: 66.0836 acc_val: 1.8391 time: 0.0126s\n",
            "Epoch: 0034 loss_train: 71.2313 acc_train: 1.9986 loss_val: 65.7944 acc_val: 1.8138 time: 0.0125s\n",
            "Epoch: 0035 loss_train: 70.8488 acc_train: 1.9601 loss_val: 65.5184 acc_val: 1.7904 time: 0.0111s\n",
            "Epoch: 0036 loss_train: 70.5650 acc_train: 1.9330 loss_val: 65.2416 acc_val: 1.7676 time: 0.0144s\n",
            "Epoch: 0037 loss_train: 70.3317 acc_train: 1.9137 loss_val: 64.9574 acc_val: 1.7450 time: 0.0123s\n",
            "Epoch: 0038 loss_train: 70.0745 acc_train: 1.8912 loss_val: 64.6105 acc_val: 1.7180 time: 0.0123s\n",
            "Epoch: 0039 loss_train: 69.8475 acc_train: 1.8730 loss_val: 64.2963 acc_val: 1.6943 time: 0.0121s\n",
            "Epoch: 0040 loss_train: 69.2212 acc_train: 1.8158 loss_val: 63.9496 acc_val: 1.6690 time: 0.0116s\n",
            "Epoch: 0041 loss_train: 68.9945 acc_train: 1.7992 loss_val: 63.6464 acc_val: 1.6474 time: 0.0135s\n",
            "Epoch: 0042 loss_train: 68.9822 acc_train: 1.8050 loss_val: 63.3723 acc_val: 1.6283 time: 0.0117s\n",
            "Epoch: 0043 loss_train: 68.5648 acc_train: 1.7704 loss_val: 63.0408 acc_val: 1.6059 time: 0.0111s\n",
            "Epoch: 0044 loss_train: 68.2467 acc_train: 1.7469 loss_val: 62.7081 acc_val: 1.5838 time: 0.0139s\n",
            "Epoch: 0045 loss_train: 67.9046 acc_train: 1.7217 loss_val: 62.2788 acc_val: 1.5561 time: 0.0114s\n",
            "Epoch: 0046 loss_train: 67.5698 acc_train: 1.6987 loss_val: 61.9237 acc_val: 1.5338 time: 0.0117s\n",
            "Epoch: 0047 loss_train: 67.3979 acc_train: 1.6900 loss_val: 61.5339 acc_val: 1.5098 time: 0.0112s\n",
            "Epoch: 0048 loss_train: 66.8731 acc_train: 1.6516 loss_val: 61.1433 acc_val: 1.4865 time: 0.0112s\n",
            "Epoch: 0049 loss_train: 66.7594 acc_train: 1.6476 loss_val: 60.8176 acc_val: 1.4676 time: 0.0115s\n",
            "Epoch: 0050 loss_train: 66.3408 acc_train: 1.6202 loss_val: 60.4088 acc_val: 1.4446 time: 0.0132s\n",
            "Epoch: 0051 loss_train: 65.7328 acc_train: 1.5772 loss_val: 59.8902 acc_val: 1.4161 time: 0.0126s\n",
            "Epoch: 0052 loss_train: 65.5614 acc_train: 1.5711 loss_val: 59.5800 acc_val: 1.3997 time: 0.0136s\n",
            "Epoch: 0053 loss_train: 65.1023 acc_train: 1.5418 loss_val: 59.2265 acc_val: 1.3813 time: 0.0134s\n",
            "Epoch: 0054 loss_train: 65.0925 acc_train: 1.5476 loss_val: 58.8392 acc_val: 1.3616 time: 0.0149s\n",
            "Epoch: 0055 loss_train: 64.6903 acc_train: 1.5219 loss_val: 58.5449 acc_val: 1.3470 time: 0.0141s\n",
            "Epoch: 0056 loss_train: 64.4762 acc_train: 1.5126 loss_val: 58.2642 acc_val: 1.3332 time: 0.0136s\n",
            "Epoch: 0057 loss_train: 64.5126 acc_train: 1.5203 loss_val: 57.9844 acc_val: 1.3197 time: 0.0122s\n",
            "Epoch: 0058 loss_train: 63.8908 acc_train: 1.4810 loss_val: 57.7082 acc_val: 1.3066 time: 0.0128s\n",
            "Epoch: 0059 loss_train: 63.5169 acc_train: 1.4594 loss_val: 57.4360 acc_val: 1.2939 time: 0.0113s\n",
            "Epoch: 0060 loss_train: 63.4199 acc_train: 1.4572 loss_val: 57.1644 acc_val: 1.2814 time: 0.0113s\n",
            "Epoch: 0061 loss_train: 63.2588 acc_train: 1.4500 loss_val: 56.8990 acc_val: 1.2693 time: 0.0123s\n",
            "Epoch: 0062 loss_train: 63.1371 acc_train: 1.4458 loss_val: 56.6385 acc_val: 1.2575 time: 0.0108s\n",
            "Epoch: 0063 loss_train: 62.5395 acc_train: 1.4102 loss_val: 56.3786 acc_val: 1.2461 time: 0.0111s\n",
            "Epoch: 0064 loss_train: 62.1301 acc_train: 1.3882 loss_val: 56.0069 acc_val: 1.2298 time: 0.0129s\n",
            "Epoch: 0065 loss_train: 61.9863 acc_train: 1.3834 loss_val: 55.6828 acc_val: 1.2160 time: 0.0129s\n",
            "Epoch: 0066 loss_train: 61.7128 acc_train: 1.3706 loss_val: 55.4340 acc_val: 1.2055 time: 0.0138s\n",
            "Epoch: 0067 loss_train: 61.8491 acc_train: 1.3832 loss_val: 55.1911 acc_val: 1.1953 time: 0.0118s\n",
            "Epoch: 0068 loss_train: 61.5055 acc_train: 1.3655 loss_val: 54.9493 acc_val: 1.1853 time: 0.0109s\n",
            "Epoch: 0069 loss_train: 61.1795 acc_train: 1.3495 loss_val: 54.7132 acc_val: 1.1757 time: 0.0109s\n",
            "Epoch: 0070 loss_train: 60.8691 acc_train: 1.3339 loss_val: 54.4823 acc_val: 1.1663 time: 0.0112s\n",
            "Epoch: 0071 loss_train: 60.5179 acc_train: 1.3164 loss_val: 54.2525 acc_val: 1.1571 time: 0.0130s\n",
            "Epoch: 0072 loss_train: 60.4250 acc_train: 1.3144 loss_val: 54.0256 acc_val: 1.1482 time: 0.0111s\n",
            "Epoch: 0073 loss_train: 60.2063 acc_train: 1.3050 loss_val: 53.8014 acc_val: 1.1395 time: 0.0124s\n",
            "Epoch: 0074 loss_train: 59.7785 acc_train: 1.2841 loss_val: 53.5781 acc_val: 1.1311 time: 0.0115s\n",
            "Epoch: 0075 loss_train: 59.7778 acc_train: 1.2871 loss_val: 53.3611 acc_val: 1.1229 time: 0.0110s\n",
            "Epoch: 0076 loss_train: 59.3630 acc_train: 1.2670 loss_val: 53.1503 acc_val: 1.1149 time: 0.0113s\n",
            "Epoch: 0077 loss_train: 59.3744 acc_train: 1.2707 loss_val: 52.9407 acc_val: 1.1071 time: 0.0117s\n",
            "Epoch: 0078 loss_train: 58.7977 acc_train: 1.2418 loss_val: 52.7334 acc_val: 1.0996 time: 0.0108s\n",
            "Epoch: 0079 loss_train: 58.6376 acc_train: 1.2362 loss_val: 52.5291 acc_val: 1.0922 time: 0.0112s\n",
            "Epoch: 0080 loss_train: 58.8091 acc_train: 1.2486 loss_val: 52.3259 acc_val: 1.0850 time: 0.0109s\n",
            "Epoch: 0081 loss_train: 58.6316 acc_train: 1.2417 loss_val: 52.1258 acc_val: 1.0781 time: 0.0107s\n",
            "Epoch: 0082 loss_train: 58.3036 acc_train: 1.2271 loss_val: 51.9314 acc_val: 1.0713 time: 0.0117s\n",
            "Epoch: 0083 loss_train: 57.7312 acc_train: 1.2006 loss_val: 51.7378 acc_val: 1.0647 time: 0.0110s\n",
            "Epoch: 0084 loss_train: 57.6258 acc_train: 1.1980 loss_val: 51.5458 acc_val: 1.0583 time: 0.0107s\n",
            "Epoch: 0085 loss_train: 57.4185 acc_train: 1.1897 loss_val: 51.3607 acc_val: 1.0520 time: 0.0109s\n",
            "Epoch: 0086 loss_train: 57.4073 acc_train: 1.1922 loss_val: 51.1768 acc_val: 1.0459 time: 0.0140s\n",
            "Epoch: 0087 loss_train: 57.7599 acc_train: 1.2118 loss_val: 50.9942 acc_val: 1.0400 time: 0.0151s\n",
            "Epoch: 0088 loss_train: 57.2352 acc_train: 1.1889 loss_val: 50.8184 acc_val: 1.0343 time: 0.0147s\n",
            "Epoch: 0089 loss_train: 56.8426 acc_train: 1.1719 loss_val: 50.6444 acc_val: 1.0287 time: 0.0114s\n",
            "Epoch: 0090 loss_train: 56.5558 acc_train: 1.1603 loss_val: 50.4714 acc_val: 1.0233 time: 0.0138s\n",
            "Epoch: 0091 loss_train: 56.8152 acc_train: 1.1748 loss_val: 50.3033 acc_val: 1.0180 time: 0.0122s\n",
            "Epoch: 0092 loss_train: 56.7339 acc_train: 1.1725 loss_val: 50.1390 acc_val: 1.0128 time: 0.0127s\n",
            "Epoch: 0093 loss_train: 56.2440 acc_train: 1.1518 loss_val: 49.9758 acc_val: 1.0079 time: 0.0118s\n",
            "Epoch: 0094 loss_train: 55.8266 acc_train: 1.1344 loss_val: 49.8151 acc_val: 1.0030 time: 0.0113s\n",
            "Epoch: 0095 loss_train: 56.1465 acc_train: 1.1511 loss_val: 49.6605 acc_val: 0.9983 time: 0.0112s\n",
            "Epoch: 0096 loss_train: 55.8291 acc_train: 1.1381 loss_val: 49.5073 acc_val: 0.9938 time: 0.0113s\n",
            "Epoch: 0097 loss_train: 56.0899 acc_train: 1.1515 loss_val: 49.3555 acc_val: 0.9893 time: 0.0115s\n",
            "Epoch: 0098 loss_train: 55.3168 acc_train: 1.1189 loss_val: 49.2095 acc_val: 0.9851 time: 0.0116s\n",
            "Epoch: 0099 loss_train: 55.2887 acc_train: 1.1191 loss_val: 49.0662 acc_val: 0.9809 time: 0.0115s\n",
            "Epoch: 0100 loss_train: 54.9287 acc_train: 1.1044 loss_val: 48.9242 acc_val: 0.9769 time: 0.0116s\n",
            "Epoch: 0101 loss_train: 54.8914 acc_train: 1.1045 loss_val: 48.7851 acc_val: 0.9729 time: 0.0138s\n",
            "Epoch: 0102 loss_train: 55.0284 acc_train: 1.1116 loss_val: 48.6532 acc_val: 0.9692 time: 0.0147s\n",
            "Epoch: 0103 loss_train: 54.7296 acc_train: 1.1000 loss_val: 48.5228 acc_val: 0.9655 time: 0.0113s\n",
            "Epoch: 0104 loss_train: 54.6171 acc_train: 1.0963 loss_val: 48.3937 acc_val: 0.9619 time: 0.0127s\n",
            "Epoch: 0105 loss_train: 54.3594 acc_train: 1.0864 loss_val: 48.2716 acc_val: 0.9585 time: 0.0114s\n",
            "Epoch: 0106 loss_train: 54.3430 acc_train: 1.0864 loss_val: 48.1563 acc_val: 0.9552 time: 0.0116s\n",
            "Epoch: 0107 loss_train: 54.3815 acc_train: 1.0893 loss_val: 48.0423 acc_val: 0.9520 time: 0.0119s\n",
            "Epoch: 0108 loss_train: 54.2009 acc_train: 1.0827 loss_val: 47.9295 acc_val: 0.9488 time: 0.0116s\n",
            "Epoch: 0109 loss_train: 54.0838 acc_train: 1.0788 loss_val: 47.8221 acc_val: 0.9458 time: 0.0123s\n",
            "Epoch: 0110 loss_train: 53.5616 acc_train: 1.0579 loss_val: 47.7186 acc_val: 0.9429 time: 0.0138s\n",
            "Epoch: 0111 loss_train: 53.6109 acc_train: 1.0606 loss_val: 47.6164 acc_val: 0.9401 time: 0.0125s\n",
            "Epoch: 0112 loss_train: 53.7327 acc_train: 1.0664 loss_val: 47.5155 acc_val: 0.9373 time: 0.0125s\n",
            "Epoch: 0113 loss_train: 53.6075 acc_train: 1.0618 loss_val: 47.4172 acc_val: 0.9347 time: 0.0128s\n",
            "Epoch: 0114 loss_train: 53.6858 acc_train: 1.0653 loss_val: 47.3211 acc_val: 0.9321 time: 0.0126s\n",
            "Epoch: 0115 loss_train: 53.3974 acc_train: 1.0545 loss_val: 47.2259 acc_val: 0.9296 time: 0.0130s\n",
            "Epoch: 0116 loss_train: 53.3585 acc_train: 1.0535 loss_val: 47.1315 acc_val: 0.9272 time: 0.0112s\n",
            "Epoch: 0117 loss_train: 53.3246 acc_train: 1.0528 loss_val: 47.0422 acc_val: 0.9249 time: 0.0121s\n",
            "Epoch: 0118 loss_train: 52.7541 acc_train: 1.0309 loss_val: 46.9606 acc_val: 0.9226 time: 0.0117s\n",
            "Epoch: 0119 loss_train: 52.9175 acc_train: 1.0381 loss_val: 46.8797 acc_val: 0.9204 time: 0.0112s\n",
            "Epoch: 0120 loss_train: 53.1486 acc_train: 1.0475 loss_val: 46.7996 acc_val: 0.9183 time: 0.0109s\n",
            "Epoch: 0121 loss_train: 53.1285 acc_train: 1.0474 loss_val: 46.7202 acc_val: 0.9162 time: 0.0113s\n",
            "Epoch: 0122 loss_train: 52.6202 acc_train: 1.0283 loss_val: 46.6481 acc_val: 0.9142 time: 0.0127s\n",
            "Epoch: 0123 loss_train: 52.8019 acc_train: 1.0359 loss_val: 46.5770 acc_val: 0.9122 time: 0.0110s\n",
            "Epoch: 0124 loss_train: 53.0741 acc_train: 1.0466 loss_val: 46.5065 acc_val: 0.9103 time: 0.0114s\n",
            "Epoch: 0125 loss_train: 52.4657 acc_train: 1.0239 loss_val: 46.4366 acc_val: 0.9085 time: 0.0110s\n",
            "Epoch: 0126 loss_train: 52.5290 acc_train: 1.0267 loss_val: 46.3688 acc_val: 0.9067 time: 0.0108s\n",
            "Epoch: 0127 loss_train: 52.4669 acc_train: 1.0248 loss_val: 46.3046 acc_val: 0.9050 time: 0.0109s\n",
            "Epoch: 0128 loss_train: 52.0136 acc_train: 1.0081 loss_val: 46.2410 acc_val: 0.9033 time: 0.0114s\n",
            "Epoch: 0129 loss_train: 52.4487 acc_train: 1.0248 loss_val: 46.1779 acc_val: 0.9016 time: 0.0112s\n",
            "Epoch: 0130 loss_train: 52.0012 acc_train: 1.0087 loss_val: 46.1153 acc_val: 0.9000 time: 0.0110s\n",
            "Epoch: 0131 loss_train: 52.1057 acc_train: 1.0130 loss_val: 46.0557 acc_val: 0.8985 time: 0.0110s\n",
            "Epoch: 0132 loss_train: 52.3269 acc_train: 1.0212 loss_val: 45.9987 acc_val: 0.8970 time: 0.0118s\n",
            "Epoch: 0133 loss_train: 52.1068 acc_train: 1.0136 loss_val: 45.9425 acc_val: 0.8956 time: 0.0116s\n",
            "Epoch: 0134 loss_train: 52.3582 acc_train: 1.0226 loss_val: 45.8871 acc_val: 0.8942 time: 0.0146s\n",
            "Epoch: 0135 loss_train: 51.8335 acc_train: 1.0039 loss_val: 45.8323 acc_val: 0.8928 time: 0.0113s\n",
            "Epoch: 0136 loss_train: 51.8119 acc_train: 1.0033 loss_val: 45.7814 acc_val: 0.8915 time: 0.0153s\n",
            "Epoch: 0137 loss_train: 51.3051 acc_train: 0.9848 loss_val: 45.7340 acc_val: 0.8903 time: 0.0140s\n",
            "Epoch: 0138 loss_train: 51.4239 acc_train: 0.9895 loss_val: 45.6873 acc_val: 0.8891 time: 0.0154s\n",
            "Epoch: 0139 loss_train: 51.4177 acc_train: 0.9893 loss_val: 45.6412 acc_val: 0.8879 time: 0.0110s\n",
            "Epoch: 0140 loss_train: 51.5986 acc_train: 0.9961 loss_val: 45.5956 acc_val: 0.8868 time: 0.0106s\n",
            "Epoch: 0141 loss_train: 51.1808 acc_train: 0.9812 loss_val: 45.5514 acc_val: 0.8857 time: 0.0106s\n",
            "Epoch: 0142 loss_train: 51.6918 acc_train: 0.9996 loss_val: 45.5144 acc_val: 0.8846 time: 0.0111s\n",
            "Epoch: 0143 loss_train: 51.5820 acc_train: 0.9958 loss_val: 45.4779 acc_val: 0.8836 time: 0.0113s\n",
            "Epoch: 0144 loss_train: 51.3963 acc_train: 0.9892 loss_val: 45.4418 acc_val: 0.8826 time: 0.0111s\n",
            "Epoch: 0145 loss_train: 51.2198 acc_train: 0.9831 loss_val: 45.4062 acc_val: 0.8816 time: 0.0109s\n",
            "Epoch: 0146 loss_train: 51.3267 acc_train: 0.9868 loss_val: 45.3709 acc_val: 0.8807 time: 0.0112s\n",
            "Epoch: 0147 loss_train: 50.9016 acc_train: 0.9719 loss_val: 45.3372 acc_val: 0.8798 time: 0.0112s\n",
            "Epoch: 0148 loss_train: 51.2395 acc_train: 0.9839 loss_val: 45.3063 acc_val: 0.8789 time: 0.0133s\n",
            "Epoch: 0149 loss_train: 51.0873 acc_train: 0.9786 loss_val: 45.2758 acc_val: 0.8781 time: 0.0126s\n",
            "Epoch: 0150 loss_train: 51.0841 acc_train: 0.9786 loss_val: 45.2456 acc_val: 0.8772 time: 0.0125s\n",
            "Epoch: 0151 loss_train: 51.1624 acc_train: 0.9813 loss_val: 45.2157 acc_val: 0.8765 time: 0.0124s\n",
            "Epoch: 0152 loss_train: 50.8680 acc_train: 0.9709 loss_val: 45.1861 acc_val: 0.8757 time: 0.0110s\n",
            "Epoch: 0153 loss_train: 51.1762 acc_train: 0.9818 loss_val: 45.1571 acc_val: 0.8749 time: 0.0117s\n",
            "Epoch: 0154 loss_train: 50.9560 acc_train: 0.9742 loss_val: 45.1305 acc_val: 0.8742 time: 0.0121s\n",
            "Epoch: 0155 loss_train: 50.6911 acc_train: 0.9648 loss_val: 45.1043 acc_val: 0.8735 time: 0.0108s\n",
            "Epoch: 0156 loss_train: 50.9448 acc_train: 0.9738 loss_val: 45.0783 acc_val: 0.8729 time: 0.0121s\n",
            "Epoch: 0157 loss_train: 50.7293 acc_train: 0.9665 loss_val: 45.0526 acc_val: 0.8722 time: 0.0127s\n",
            "Epoch: 0158 loss_train: 50.6440 acc_train: 0.9635 loss_val: 45.0272 acc_val: 0.8716 time: 0.0123s\n",
            "Epoch: 0159 loss_train: 50.5781 acc_train: 0.9613 loss_val: 45.0019 acc_val: 0.8710 time: 0.0122s\n",
            "Epoch: 0160 loss_train: 50.5852 acc_train: 0.9616 loss_val: 44.9792 acc_val: 0.8704 time: 0.0112s\n",
            "Epoch: 0161 loss_train: 50.4769 acc_train: 0.9579 loss_val: 44.9591 acc_val: 0.8698 time: 0.0111s\n",
            "Epoch: 0162 loss_train: 50.8684 acc_train: 0.9710 loss_val: 44.9391 acc_val: 0.8692 time: 0.0111s\n",
            "Epoch: 0163 loss_train: 50.8887 acc_train: 0.9720 loss_val: 44.9194 acc_val: 0.8687 time: 0.0124s\n",
            "Epoch: 0164 loss_train: 50.6916 acc_train: 0.9653 loss_val: 44.8997 acc_val: 0.8682 time: 0.0119s\n",
            "Epoch: 0165 loss_train: 50.6822 acc_train: 0.9648 loss_val: 44.8803 acc_val: 0.8676 time: 0.0133s\n",
            "Epoch: 0166 loss_train: 50.6060 acc_train: 0.9624 loss_val: 44.8609 acc_val: 0.8672 time: 0.0120s\n",
            "Epoch: 0167 loss_train: 50.3680 acc_train: 0.9541 loss_val: 44.8438 acc_val: 0.8667 time: 0.0122s\n",
            "Epoch: 0168 loss_train: 50.4320 acc_train: 0.9565 loss_val: 44.8299 acc_val: 0.8662 time: 0.0116s\n",
            "Epoch: 0169 loss_train: 50.5728 acc_train: 0.9611 loss_val: 44.8160 acc_val: 0.8658 time: 0.0115s\n",
            "Epoch: 0170 loss_train: 50.3989 acc_train: 0.9553 loss_val: 44.8023 acc_val: 0.8653 time: 0.0113s\n",
            "Epoch: 0171 loss_train: 50.3529 acc_train: 0.9539 loss_val: 44.7887 acc_val: 0.8649 time: 0.0109s\n",
            "Epoch: 0172 loss_train: 50.1442 acc_train: 0.9468 loss_val: 44.7752 acc_val: 0.8645 time: 0.0111s\n",
            "Epoch: 0173 loss_train: 50.3716 acc_train: 0.9544 loss_val: 44.7617 acc_val: 0.8641 time: 0.0109s\n",
            "Epoch: 0174 loss_train: 50.2722 acc_train: 0.9510 loss_val: 44.7484 acc_val: 0.8637 time: 0.0120s\n",
            "Epoch: 0175 loss_train: 50.2283 acc_train: 0.9497 loss_val: 44.7375 acc_val: 0.8633 time: 0.0119s\n",
            "Epoch: 0176 loss_train: 49.9955 acc_train: 0.9419 loss_val: 44.7271 acc_val: 0.8630 time: 0.0109s\n",
            "Epoch: 0177 loss_train: 50.3690 acc_train: 0.9543 loss_val: 44.7167 acc_val: 0.8626 time: 0.0110s\n",
            "Epoch: 0178 loss_train: 50.4686 acc_train: 0.9575 loss_val: 44.7064 acc_val: 0.8623 time: 0.0117s\n",
            "Epoch: 0179 loss_train: 50.3188 acc_train: 0.9526 loss_val: 44.6962 acc_val: 0.8620 time: 0.0109s\n",
            "Epoch: 0180 loss_train: 50.0188 acc_train: 0.9427 loss_val: 44.6860 acc_val: 0.8617 time: 0.0122s\n",
            "Epoch: 0181 loss_train: 50.0269 acc_train: 0.9431 loss_val: 44.6760 acc_val: 0.8613 time: 0.0113s\n",
            "Epoch: 0182 loss_train: 50.0689 acc_train: 0.9444 loss_val: 44.6659 acc_val: 0.8610 time: 0.0126s\n",
            "Epoch: 0183 loss_train: 50.2257 acc_train: 0.9496 loss_val: 44.6582 acc_val: 0.8608 time: 0.0111s\n",
            "Epoch: 0184 loss_train: 49.9403 acc_train: 0.9402 loss_val: 44.6509 acc_val: 0.8605 time: 0.0122s\n",
            "Epoch: 0185 loss_train: 50.2769 acc_train: 0.9512 loss_val: 44.6437 acc_val: 0.8602 time: 0.0109s\n",
            "Epoch: 0186 loss_train: 49.7766 acc_train: 0.9347 loss_val: 44.6366 acc_val: 0.8600 time: 0.0112s\n",
            "Epoch: 0187 loss_train: 50.2122 acc_train: 0.9489 loss_val: 44.6296 acc_val: 0.8597 time: 0.0111s\n",
            "Epoch: 0188 loss_train: 49.8631 acc_train: 0.9375 loss_val: 44.6226 acc_val: 0.8595 time: 0.0109s\n",
            "Epoch: 0189 loss_train: 50.2356 acc_train: 0.9495 loss_val: 44.6157 acc_val: 0.8593 time: 0.0109s\n",
            "Epoch: 0190 loss_train: 50.0131 acc_train: 0.9423 loss_val: 44.6089 acc_val: 0.8590 time: 0.0109s\n",
            "Epoch: 0191 loss_train: 49.7064 acc_train: 0.9323 loss_val: 44.6025 acc_val: 0.8588 time: 0.0112s\n",
            "Epoch: 0192 loss_train: 50.2120 acc_train: 0.9486 loss_val: 44.5969 acc_val: 0.8586 time: 0.0112s\n",
            "Epoch: 0193 loss_train: 49.8675 acc_train: 0.9375 loss_val: 44.5718 acc_val: 0.8578 time: 0.0114s\n",
            "Epoch: 0194 loss_train: 50.0497 acc_train: 0.9433 loss_val: 44.5649 acc_val: 0.8576 time: 0.0150s\n",
            "Epoch: 0195 loss_train: 50.1921 acc_train: 0.9478 loss_val: 44.5595 acc_val: 0.8574 time: 0.0112s\n",
            "Epoch: 0196 loss_train: 49.3922 acc_train: 0.9218 loss_val: 44.5542 acc_val: 0.8573 time: 0.0118s\n",
            "Epoch: 0197 loss_train: 50.0530 acc_train: 0.9432 loss_val: 44.5511 acc_val: 0.8571 time: 0.0130s\n",
            "Epoch: 0198 loss_train: 49.8038 acc_train: 0.9351 loss_val: 44.5481 acc_val: 0.8570 time: 0.0110s\n",
            "Epoch: 0199 loss_train: 49.9079 acc_train: 0.9384 loss_val: 44.5452 acc_val: 0.8569 time: 0.0111s\n",
            "Epoch: 0200 loss_train: 49.7580 acc_train: 0.9335 loss_val: 44.5423 acc_val: 0.8567 time: 0.0115s\n",
            "Epoch: 0201 loss_train: 49.7852 acc_train: 0.9343 loss_val: 44.5395 acc_val: 0.8566 time: 0.0110s\n",
            "Epoch: 0202 loss_train: 49.6548 acc_train: 0.9300 loss_val: 44.5366 acc_val: 0.8565 time: 0.0130s\n",
            "Epoch: 0203 loss_train: 49.5999 acc_train: 0.9282 loss_val: 44.5339 acc_val: 0.8564 time: 0.0123s\n",
            "Epoch: 0204 loss_train: 49.7193 acc_train: 0.9320 loss_val: 44.5311 acc_val: 0.8563 time: 0.0111s\n",
            "Epoch: 0205 loss_train: 49.7566 acc_train: 0.9332 loss_val: 44.5284 acc_val: 0.8562 time: 0.0111s\n",
            "Epoch: 0206 loss_train: 49.7387 acc_train: 0.9326 loss_val: 44.5257 acc_val: 0.8561 time: 0.0111s\n",
            "Epoch: 0207 loss_train: 49.3997 acc_train: 0.9216 loss_val: 44.5238 acc_val: 0.8561 time: 0.0109s\n",
            "Epoch: 0208 loss_train: 49.4903 acc_train: 0.9244 loss_val: 44.5221 acc_val: 0.8560 time: 0.0112s\n",
            "Epoch: 0209 loss_train: 49.5779 acc_train: 0.9272 loss_val: 44.5204 acc_val: 0.8559 time: 0.0111s\n",
            "Epoch: 0210 loss_train: 49.6288 acc_train: 0.9288 loss_val: 44.5188 acc_val: 0.8558 time: 0.0113s\n",
            "Epoch: 0211 loss_train: 49.9246 acc_train: 0.9381 loss_val: 44.5172 acc_val: 0.8558 time: 0.0122s\n",
            "Epoch: 0212 loss_train: 49.3396 acc_train: 0.9194 loss_val: 44.5156 acc_val: 0.8557 time: 0.0151s\n",
            "Epoch: 0213 loss_train: 49.5879 acc_train: 0.9273 loss_val: 44.5140 acc_val: 0.8557 time: 0.0111s\n",
            "Epoch: 0214 loss_train: 49.6345 acc_train: 0.9288 loss_val: 44.5125 acc_val: 0.8556 time: 0.0116s\n",
            "Epoch: 0215 loss_train: 49.6257 acc_train: 0.9284 loss_val: 44.5109 acc_val: 0.8555 time: 0.0134s\n",
            "Epoch: 0216 loss_train: 49.4326 acc_train: 0.9222 loss_val: 44.5094 acc_val: 0.8555 time: 0.0115s\n",
            "Epoch: 0217 loss_train: 49.1571 acc_train: 0.9134 loss_val: 44.5079 acc_val: 0.8555 time: 0.0111s\n",
            "Epoch: 0218 loss_train: 49.5306 acc_train: 0.9252 loss_val: 44.5066 acc_val: 0.8554 time: 0.0117s\n",
            "Epoch: 0219 loss_train: 49.6137 acc_train: 0.9278 loss_val: 44.5064 acc_val: 0.8554 time: 0.0111s\n",
            "Epoch: 0220 loss_train: 49.6765 acc_train: 0.9298 loss_val: 44.5061 acc_val: 0.8553 time: 0.0115s\n",
            "Epoch: 0221 loss_train: 49.4754 acc_train: 0.9234 loss_val: 44.5059 acc_val: 0.8553 time: 0.0138s\n",
            "Epoch: 0222 loss_train: 49.4891 acc_train: 0.9237 loss_val: 44.5057 acc_val: 0.8553 time: 0.0124s\n",
            "Epoch: 0223 loss_train: 49.2849 acc_train: 0.9171 loss_val: 44.5055 acc_val: 0.8553 time: 0.0123s\n",
            "Epoch: 0224 loss_train: 49.3355 acc_train: 0.9188 loss_val: 44.5053 acc_val: 0.8552 time: 0.0113s\n",
            "Epoch: 0225 loss_train: 49.4823 acc_train: 0.9233 loss_val: 44.5051 acc_val: 0.8552 time: 0.0107s\n",
            "Epoch: 0226 loss_train: 49.5023 acc_train: 0.9240 loss_val: 44.5049 acc_val: 0.8552 time: 0.0115s\n",
            "Epoch: 0227 loss_train: 49.5338 acc_train: 0.9248 loss_val: 44.5047 acc_val: 0.8552 time: 0.0110s\n",
            "Epoch: 0228 loss_train: 49.4491 acc_train: 0.9222 loss_val: 44.5045 acc_val: 0.8552 time: 0.0113s\n",
            "Epoch: 0229 loss_train: 49.2796 acc_train: 0.9167 loss_val: 44.5043 acc_val: 0.8552 time: 0.0145s\n",
            "Epoch: 0230 loss_train: 49.5301 acc_train: 0.9246 loss_val: 44.5041 acc_val: 0.8552 time: 0.0126s\n",
            "Epoch: 0231 loss_train: 49.4686 acc_train: 0.9227 loss_val: 44.5040 acc_val: 0.8551 time: 0.0112s\n",
            "Epoch: 0232 loss_train: 49.3488 acc_train: 0.9189 loss_val: 44.5057 acc_val: 0.8551 time: 0.0115s\n",
            "Epoch: 0233 loss_train: 49.2803 acc_train: 0.9166 loss_val: 44.5074 acc_val: 0.8551 time: 0.0134s\n",
            "Epoch: 0234 loss_train: 49.3112 acc_train: 0.9175 loss_val: 44.5090 acc_val: 0.8551 time: 0.0112s\n",
            "Epoch: 0235 loss_train: 49.3088 acc_train: 0.9174 loss_val: 44.5106 acc_val: 0.8551 time: 0.0111s\n",
            "Epoch: 0236 loss_train: 49.4823 acc_train: 0.9227 loss_val: 44.5121 acc_val: 0.8551 time: 0.0113s\n",
            "Epoch: 0237 loss_train: 49.3296 acc_train: 0.9178 loss_val: 44.5137 acc_val: 0.8551 time: 0.0112s\n",
            "Epoch: 0238 loss_train: 49.7366 acc_train: 0.9306 loss_val: 44.5152 acc_val: 0.8552 time: 0.0120s\n",
            "Epoch: 0239 loss_train: 49.2264 acc_train: 0.9145 loss_val: 44.5168 acc_val: 0.8552 time: 0.0119s\n",
            "Epoch: 0240 loss_train: 49.4211 acc_train: 0.9205 loss_val: 44.5182 acc_val: 0.8552 time: 0.0140s\n",
            "Epoch: 0241 loss_train: 49.2441 acc_train: 0.9148 loss_val: 44.5197 acc_val: 0.8552 time: 0.0113s\n",
            "Epoch: 0242 loss_train: 49.4996 acc_train: 0.9229 loss_val: 44.5212 acc_val: 0.8552 time: 0.0109s\n",
            "Epoch: 0243 loss_train: 49.2690 acc_train: 0.9156 loss_val: 44.5227 acc_val: 0.8552 time: 0.0114s\n",
            "Epoch: 0244 loss_train: 49.0802 acc_train: 0.9095 loss_val: 44.5241 acc_val: 0.8552 time: 0.0112s\n",
            "Epoch: 0245 loss_train: 49.2386 acc_train: 0.9146 loss_val: 44.5255 acc_val: 0.8552 time: 0.0113s\n",
            "Epoch: 0246 loss_train: 49.3868 acc_train: 0.9192 loss_val: 44.5269 acc_val: 0.8553 time: 0.0117s\n",
            "Epoch: 0247 loss_train: 49.6321 acc_train: 0.9268 loss_val: 44.5289 acc_val: 0.8553 time: 0.0126s\n",
            "Epoch: 0248 loss_train: 49.2235 acc_train: 0.9140 loss_val: 44.5315 acc_val: 0.8553 time: 0.0111s\n",
            "Epoch: 0249 loss_train: 49.4390 acc_train: 0.9206 loss_val: 44.5340 acc_val: 0.8553 time: 0.0112s\n",
            "Epoch: 0250 loss_train: 49.1889 acc_train: 0.9127 loss_val: 44.5365 acc_val: 0.8553 time: 0.0109s\n",
            "Epoch: 0251 loss_train: 49.1029 acc_train: 0.9100 loss_val: 44.5388 acc_val: 0.8554 time: 0.0109s\n",
            "Epoch: 0252 loss_train: 49.2168 acc_train: 0.9134 loss_val: 44.5411 acc_val: 0.8554 time: 0.0112s\n",
            "Epoch: 0253 loss_train: 49.4659 acc_train: 0.9212 loss_val: 44.5434 acc_val: 0.8554 time: 0.0109s\n",
            "Epoch: 0254 loss_train: 49.1334 acc_train: 0.9107 loss_val: 44.5456 acc_val: 0.8554 time: 0.0110s\n",
            "Epoch: 0255 loss_train: 49.3090 acc_train: 0.9162 loss_val: 44.5478 acc_val: 0.8555 time: 0.0110s\n",
            "Epoch: 0256 loss_train: 49.5023 acc_train: 0.9221 loss_val: 44.5500 acc_val: 0.8555 time: 0.0110s\n",
            "Epoch: 0257 loss_train: 49.2645 acc_train: 0.9146 loss_val: 44.5521 acc_val: 0.8555 time: 0.0114s\n",
            "Epoch: 0258 loss_train: 49.3263 acc_train: 0.9166 loss_val: 44.5542 acc_val: 0.8555 time: 0.0112s\n",
            "Epoch: 0259 loss_train: 49.3081 acc_train: 0.9159 loss_val: 44.5563 acc_val: 0.8556 time: 0.0132s\n",
            "Epoch: 0260 loss_train: 49.1322 acc_train: 0.9104 loss_val: 44.5584 acc_val: 0.8556 time: 0.0134s\n",
            "Epoch: 0261 loss_train: 49.1172 acc_train: 0.9098 loss_val: 44.5604 acc_val: 0.8556 time: 0.0109s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 6.6697s\n",
            "Loading epoch 230\n",
            "Test set results: loss= 72.7494 accuracy= 0.9316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bINVLVwn96u"
      },
      "source": [
        "test_output = model(test_features, adj)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "YY7-srfgn96w",
        "outputId": "3df09901-98cc-45ed-bc12-1a8fbb04007c"
      },
      "source": [
        "print(stats.pearsonr(train_labels.cpu().detach().numpy(),train_output.cpu().detach().numpy()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8fa9faed94ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36mpearsonr\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   3517\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3519\u001b[0;31m     \u001b[0mxmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3520\u001b[0m     \u001b[0mymean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         ret = um.true_divide(\n",
            "\u001b[0;31mTypeError\u001b[0m: No loop matching the specified signature and casting was found for ufunc add"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ0MQVw3TPGR"
      },
      "source": [
        "## Train Two Layer basic GAT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 875
        },
        "id": "eETbR1Y9TU9R",
        "outputId": "8e2d4b34-11b8-4b04-80c2-836eebc651fd"
      },
      "source": [
        "# Training settings\n",
        "nocuda = False\n",
        "fastmode = False\n",
        "sparse = False\n",
        "seed = 72\n",
        "epochs = 1000\n",
        "lr = 0.005\n",
        "weight_decay = 5e-3\n",
        "hidden = 8\n",
        "nb_heads = 8\n",
        "dropout = 0.2\n",
        "alpha = 0.2\n",
        "patience = 30\n",
        "nclass = int(1)\n",
        "\n",
        "cuda = not nocuda and torch.cuda.is_available()\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Load data\n",
        "adj, train_features, train_labels, valid_features, valid_labels, test_features, test_labels = load_data()\n",
        "\n",
        "# Model and optimizer\n",
        "model = TwoLayerGATMLP(nfeat=train_features.shape[1],\n",
        "                nhid=hidden,\n",
        "                nclass=nclass,\n",
        "                dropout=dropout,\n",
        "                nheads=nb_heads,\n",
        "                alpha=alpha)\n",
        "optimizer = optim.Adam(model.parameters(), \n",
        "                       lr=lr, \n",
        "                       weight_decay=weight_decay)\n",
        "print(model)\n",
        "\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    adj = adj.cuda()\n",
        "    train_features = train_features.cuda()\n",
        "    valid_features = valid_features.cuda()\n",
        "    test_features = test_features.cuda()\n",
        "    train_labels = train_labels.cuda()\n",
        "    valid_labels = valid_labels.cuda()\n",
        "    test_labels = test_labels.cuda()\n",
        "\n",
        "adj, train_features, valid_features, test_features, train_labels, valid_labels, test_labels = Variable(adj), Variable(train_features), Variable(valid_features), Variable(test_features), Variable(train_labels), Variable(valid_labels), Variable(test_labels)\n",
        "loss = nn.L1Loss()\n",
        "acc = nn.MSELoss()\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    train_output = model(train_features, adj)\n",
        "    loss_train = loss(train_output, train_labels)\n",
        "    acc_train = torch.sqrt(acc(torch.log(train_output+1), torch.log(train_labels+1)))\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        valid_output = model(valid_features, adj)\n",
        "\n",
        "        loss_val = loss(valid_output, valid_labels)\n",
        "        acc_val = torch.sqrt(acc(torch.log(valid_output+1), torch.log(valid_labels+1)))\n",
        "        print('Epoch: {:04d}'.format(epoch+1),\n",
        "              'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "              'acc_train: {:.4f}'.format(acc_train),\n",
        "              'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "              'acc_val: {:.4f}'.format(acc_val),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_val.data.item()\n",
        "\n",
        "\n",
        "def compute_test():\n",
        "    model.eval()\n",
        "    test_output = model(test_features, adj)\n",
        "    loss_test = loss(test_output, test_labels)\n",
        "    acc_test = torch.sqrt(acc(torch.log(test_output+1), torch.log(test_labels+1)))\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "loss_values = []\n",
        "bad_counter = 0\n",
        "best = epochs + 1\n",
        "best_epoch = 0\n",
        "for epoch in range(epochs):\n",
        "    loss_values.append(train(epoch))\n",
        "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "    if loss_values[-1] < best:\n",
        "        best = loss_values[-1]\n",
        "        best_epoch = epoch\n",
        "        bad_counter = 0\n",
        "    else:\n",
        "        bad_counter += 1\n",
        "\n",
        "    if bad_counter == patience:\n",
        "        break\n",
        "\n",
        "    files = glob.glob('*.pkl')\n",
        "    for file in files:\n",
        "        epoch_nb = int(file.split('.')[0])\n",
        "        if epoch_nb < best_epoch:\n",
        "            open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "            os.remove(file)\n",
        "\n",
        "files = glob.glob('*.pkl')\n",
        "for file in files:\n",
        "    epoch_nb = int(file.split('.')[0])\n",
        "    if epoch_nb > best_epoch:\n",
        "        open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "        os.remove(file)\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Restore best model;\n",
        "print('Loading epoch {}'.format(best_epoch))\n",
        "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "# Testing\n",
        "compute_test()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading covid dataset...\n",
            "TwoLayerGATMLP(\n",
            "  (attention_0): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_1): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_2): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_3): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_4): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_5): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_6): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_7): GraphAttentionLayer (276 -> 8)\n",
            "  (out_att): GraphAttentionLayer (64 -> 64)\n",
            "  (attention2_0): GraphAttentionLayer (64 -> 4)\n",
            "  (attention2_1): GraphAttentionLayer (64 -> 4)\n",
            "  (attention2_2): GraphAttentionLayer (64 -> 4)\n",
            "  (attention2_3): GraphAttentionLayer (64 -> 4)\n",
            "  (attention2_4): GraphAttentionLayer (64 -> 4)\n",
            "  (attention2_5): GraphAttentionLayer (64 -> 4)\n",
            "  (attention2_6): GraphAttentionLayer (64 -> 4)\n",
            "  (attention2_7): GraphAttentionLayer (64 -> 4)\n",
            "  (out_att2): GraphAttentionLayer (32 -> 32)\n",
            "  (lin1): Linear(in_features=12128, out_features=6056, bias=True)\n",
            "  (lin2): Linear(in_features=6056, out_features=6056, bias=True)\n",
            "  (lin3): Linear(in_features=6056, out_features=3028, bias=True)\n",
            "  (lin4): Linear(in_features=3028, out_features=379, bias=True)\n",
            ")\n",
            "Epoch: 0001 loss_train: 79.5095 acc_train: 3.9860 loss_val: 72.8631 acc_val: 3.2546 time: 0.2398s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1aebd136bc83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mloss_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloss_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;31m# .cpu() on the underlying Storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;34m\"\"\"Returns a CPU copy of this storage if it's not already on the CPU\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;31m# or on typing_extensions module on Python >= 3.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, dtype, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nszejy9xn9EU"
      },
      "source": [
        "test_output = model(test_features, adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kzXkjM8n9EW"
      },
      "source": [
        "print(stats.pearsonr(test_labels.cpu().detach().numpy(),test_output.cpu().detach().numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdZqODYiqtDt"
      },
      "source": [
        "## Train Three Layer GAT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "id": "MPDLrZser3M7",
        "outputId": "7ec02f6f-ee47-4bbe-80db-77108b879953"
      },
      "source": [
        "# Training settings\n",
        "nocuda = False\n",
        "fastmode = False\n",
        "sparse = False\n",
        "seed = 72\n",
        "epochs = 1000\n",
        "lr = 0.005\n",
        "weight_decay = 5e-3\n",
        "hidden = 8\n",
        "nb_heads = 8\n",
        "dropout = 0.2\n",
        "alpha = 0.2\n",
        "patience = 30\n",
        "nclass = int(1)\n",
        "\n",
        "cuda = not nocuda and torch.cuda.is_available()\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Load data\n",
        "adj, train_features, train_labels, valid_features, valid_labels, test_features, test_labels = load_data()\n",
        "\n",
        "# Model and optimizer\n",
        "model = ThreeLayerGAT(nfeat=train_features.shape[1],\n",
        "                nhid=hidden,\n",
        "                nclass=nclass,\n",
        "                dropout=dropout,\n",
        "                nheads=nb_heads,\n",
        "                alpha=alpha)\n",
        "optimizer = optim.Adam(model.parameters(), \n",
        "                       lr=lr, \n",
        "                       weight_decay=weight_decay)\n",
        "print(model)\n",
        "\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    adj = adj.cuda()\n",
        "    train_features = train_features.cuda()\n",
        "    valid_features = valid_features.cuda()\n",
        "    test_features = test_features.cuda()\n",
        "    train_labels = train_labels.cuda()\n",
        "    valid_labels = valid_labels.cuda()\n",
        "    test_labels = test_labels.cuda()\n",
        "\n",
        "adj, train_features, valid_features, test_features, train_labels, valid_labels, test_labels = Variable(adj), Variable(train_features), Variable(valid_features), Variable(test_features), Variable(train_labels), Variable(valid_labels), Variable(test_labels)\n",
        "loss = nn.L1Loss()\n",
        "acc = nn.MSELoss()\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    train_output = model(train_features, adj)\n",
        "    loss_train = loss(train_output, train_labels)\n",
        "    acc_train = torch.sqrt(acc(torch.log(train_output+1), torch.log(train_labels+1)))\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        valid_output = model(valid_features, adj)\n",
        "\n",
        "        loss_val = loss(valid_output, valid_labels)\n",
        "        acc_val = torch.sqrt(acc(torch.log(valid_output+1), torch.log(valid_labels+1)))\n",
        "        print('Epoch: {:04d}'.format(epoch+1),\n",
        "              'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "              'acc_train: {:.4f}'.format(acc_train),\n",
        "              'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "              'acc_val: {:.4f}'.format(acc_val),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_val.data.item()\n",
        "\n",
        "\n",
        "def compute_test():\n",
        "    model.eval()\n",
        "    test_output = model(test_features, adj)\n",
        "    loss_test = loss(test_output, test_labels)\n",
        "    acc_test = torch.sqrt(acc(torch.log(test_output+1), torch.log(test_labels+1)))\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "loss_values = []\n",
        "bad_counter = 0\n",
        "best = epochs + 1\n",
        "best_epoch = 0\n",
        "for epoch in range(epochs):\n",
        "    loss_values.append(train(epoch))\n",
        "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "    if loss_values[-1] < best:\n",
        "        best = loss_values[-1]\n",
        "        best_epoch = epoch\n",
        "        bad_counter = 0\n",
        "    else:\n",
        "        bad_counter += 1\n",
        "\n",
        "    if bad_counter == patience:\n",
        "        break\n",
        "\n",
        "    files = glob.glob('*.pkl')\n",
        "    for file in files:\n",
        "        epoch_nb = int(file.split('.')[0])\n",
        "        if epoch_nb < best_epoch:\n",
        "            open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "            os.remove(file)\n",
        "\n",
        "files = glob.glob('*.pkl')\n",
        "for file in files:\n",
        "    epoch_nb = int(file.split('.')[0])\n",
        "    if epoch_nb > best_epoch:\n",
        "        open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "        os.remove(file)\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Restore best model;\n",
        "print('Loading epoch {}'.format(best_epoch))\n",
        "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "# Testing\n",
        "compute_test()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading covid dataset...\n",
            "ThreeLayerGAT(\n",
            "  (attention_0): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_1): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_2): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_3): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_4): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_5): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_6): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_7): GraphAttentionLayer (276 -> 8)\n",
            "  (out_att): GraphAttentionLayer (64 -> 64)\n",
            "  (attention2_0): GraphAttentionLayer (32 -> 2)\n",
            "  (attention2_1): GraphAttentionLayer (32 -> 2)\n",
            "  (attention2_2): GraphAttentionLayer (32 -> 2)\n",
            "  (attention2_3): GraphAttentionLayer (32 -> 2)\n",
            "  (attention2_4): GraphAttentionLayer (32 -> 2)\n",
            "  (attention2_5): GraphAttentionLayer (32 -> 2)\n",
            "  (attention2_6): GraphAttentionLayer (32 -> 2)\n",
            "  (attention2_7): GraphAttentionLayer (32 -> 2)\n",
            "  (out_att2): GraphAttentionLayer (32 -> 32)\n",
            "  (lin1): Linear(in_features=16, out_features=379, bias=True)\n",
            "  (out_att3): GraphAttentionLayer (16 -> 16)\n",
            ")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f393d1dfc929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mloss_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloss_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f393d1dfc929>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mtrain_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_output\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/COVID_GAT/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0matt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_att3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;31m# x = torch.flatten(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/COVID_GAT/models.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0matt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_att3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;31m# x = torch.flatten(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/COVID_GAT/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, adj)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mWh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# h.shape: (N, in_features), Wh.shape: (N, out_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_attentional_mechanism_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument mat2 in method wrapper_mm)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEZN3sI1saWC",
        "outputId": "1512e878-1454-4578-afdb-03cb0c52d544"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcQjkdxLq0Jm"
      },
      "source": [
        "test_output = model(test_features, adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuf3CUKaq0Jo"
      },
      "source": [
        "print(stats.pearsonr(test_labels.cpu().detach().numpy(),test_output.cpu().detach().numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgDLTd8yviBI"
      },
      "source": [
        "##Basic MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8-1Obudviv2",
        "outputId": "f168c680-c618-4248-997c-ec40b69e03e5"
      },
      "source": [
        "# Training settings\n",
        "nocuda = False\n",
        "fastmode = False\n",
        "sparse = False\n",
        "seed = 72\n",
        "epochs = 1000\n",
        "lr = 0.005\n",
        "weight_decay = 5e-3\n",
        "hidden = 8\n",
        "nb_heads = 8\n",
        "dropout = 0.2\n",
        "alpha = 0.2\n",
        "patience = 30\n",
        "nclass = int(1)\n",
        "\n",
        "cuda = not nocuda and torch.cuda.is_available()\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Load data\n",
        "adj, train_features, train_labels, valid_features, valid_labels, test_features, test_labels = load_data()\n",
        "\n",
        "# Model and optimizer\n",
        "model = MLP(nfeat=train_features.shape[1],\n",
        "                dropout=dropout)\n",
        "optimizer = optim.Adam(model.parameters(), \n",
        "                       lr=lr, \n",
        "                       weight_decay=weight_decay)\n",
        "print(model)\n",
        "\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    adj = adj.cuda()\n",
        "    train_features = train_features.cuda()\n",
        "    valid_features = valid_features.cuda()\n",
        "    test_features = test_features.cuda()\n",
        "    train_labels = train_labels.cuda()\n",
        "    valid_labels = valid_labels.cuda()\n",
        "    test_labels = test_labels.cuda()\n",
        "\n",
        "adj, train_features, valid_features, test_features, train_labels, valid_labels, test_labels = Variable(adj), Variable(train_features), Variable(valid_features), Variable(test_features), Variable(train_labels), Variable(valid_labels), Variable(test_labels)\n",
        "loss = nn.L1Loss()\n",
        "acc = nn.MSELoss()\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    train_output = model(train_features, adj)\n",
        "    loss_train = loss(train_output, train_labels)\n",
        "    acc_train = torch.sqrt(acc(torch.log(train_output+1), torch.log(train_labels+1)))\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        valid_output = model(valid_features, adj)\n",
        "\n",
        "        loss_val = loss(valid_output, valid_labels)\n",
        "        acc_val = torch.sqrt(acc(torch.log(valid_output+1), torch.log(valid_labels+1)))\n",
        "        print('Epoch: {:04d}'.format(epoch+1),\n",
        "              'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "              'acc_train: {:.4f}'.format(acc_train),\n",
        "              'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "              'acc_val: {:.4f}'.format(acc_val),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_val.data.item()\n",
        "\n",
        "\n",
        "def compute_test():\n",
        "    model.eval()\n",
        "    test_output = model(test_features, adj)\n",
        "    loss_test = loss(test_output, test_labels)\n",
        "    acc_test = torch.sqrt(acc(torch.log(test_output+1), torch.log(test_labels+1)))\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "loss_values = []\n",
        "bad_counter = 0\n",
        "best = epochs + 1\n",
        "best_epoch = 0\n",
        "for epoch in range(epochs):\n",
        "    loss_values.append(train(epoch))\n",
        "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "    if loss_values[-1] < best:\n",
        "        best = loss_values[-1]\n",
        "        best_epoch = epoch\n",
        "        bad_counter = 0\n",
        "    else:\n",
        "        bad_counter += 1\n",
        "\n",
        "    if bad_counter == patience:\n",
        "        break\n",
        "\n",
        "    files = glob.glob('*.pkl')\n",
        "    for file in files:\n",
        "        epoch_nb = int(file.split('.')[0])\n",
        "        if epoch_nb < best_epoch:\n",
        "            open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "            os.remove(file)\n",
        "\n",
        "files = glob.glob('*.pkl')\n",
        "for file in files:\n",
        "    epoch_nb = int(file.split('.')[0])\n",
        "    if epoch_nb > best_epoch:\n",
        "        open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "        os.remove(file)\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Restore best model;\n",
        "print('Loading epoch {}'.format(best_epoch))\n",
        "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "# Testing\n",
        "compute_test()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading covid dataset...\n",
            "MLP(\n",
            "  (lin1): Linear(in_features=276, out_features=12112, bias=True)\n",
            "  (lin2): Linear(in_features=12112, out_features=6056, bias=True)\n",
            "  (lin3): Linear(in_features=6056, out_features=3028, bias=True)\n",
            "  (lin4): Linear(in_features=3028, out_features=379, bias=True)\n",
            ")\n",
            "Epoch: 0001 loss_train: 76.8980 acc_train: 4.0183 loss_val: 52.3745 acc_val: 2.9247 time: 0.0383s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:97: UserWarning: Using a target size (torch.Size([379])) that is different to the input size (torch.Size([379, 379])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([379])) that is different to the input size (torch.Size([379, 379])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0002 loss_train: 72.1724 acc_train: 3.1714 loss_val: 46.8092 acc_val: 2.5362 time: 0.0388s\n",
            "Epoch: 0003 loss_train: 65.5670 acc_train: 2.7386 loss_val: 43.1076 acc_val: 2.2705 time: 0.0388s\n",
            "Epoch: 0004 loss_train: 59.3439 acc_train: 2.4387 loss_val: 39.8034 acc_val: 1.8313 time: 0.0389s\n",
            "Epoch: 0005 loss_train: 53.4818 acc_train: 1.9744 loss_val: 36.4272 acc_val: 1.3474 time: 0.0389s\n",
            "Epoch: 0006 loss_train: 47.6920 acc_train: 1.4560 loss_val: 32.9934 acc_val: 0.9553 time: 0.0391s\n",
            "Epoch: 0007 loss_train: 42.2886 acc_train: 1.0269 loss_val: 29.6954 acc_val: 0.7719 time: 0.0388s\n",
            "Epoch: 0008 loss_train: 37.0405 acc_train: 0.8073 loss_val: 26.3812 acc_val: 0.6611 time: 0.0389s\n",
            "Epoch: 0009 loss_train: 32.6134 acc_train: 0.6756 loss_val: 25.6006 acc_val: 0.5906 time: 0.0390s\n",
            "Epoch: 0010 loss_train: 28.8025 acc_train: 0.5449 loss_val: 24.4988 acc_val: 0.5518 time: 0.0389s\n",
            "Epoch: 0011 loss_train: 25.5142 acc_train: 0.4749 loss_val: 20.6422 acc_val: 0.4827 time: 0.0388s\n",
            "Epoch: 0012 loss_train: 23.7702 acc_train: 0.4687 loss_val: 22.8301 acc_val: 0.5043 time: 0.0388s\n",
            "Epoch: 0013 loss_train: 20.9601 acc_train: 0.4101 loss_val: 23.0056 acc_val: 0.5045 time: 0.0388s\n",
            "Epoch: 0014 loss_train: 18.8511 acc_train: 0.3832 loss_val: 21.9660 acc_val: 0.4894 time: 0.0389s\n",
            "Epoch: 0015 loss_train: 17.0706 acc_train: 0.3610 loss_val: 21.7157 acc_val: 0.4892 time: 0.0388s\n",
            "Epoch: 0016 loss_train: 15.3677 acc_train: 0.3483 loss_val: 21.2991 acc_val: 0.4907 time: 0.0389s\n",
            "Epoch: 0017 loss_train: 13.9981 acc_train: 0.3414 loss_val: 17.1412 acc_val: 0.4440 time: 0.0389s\n",
            "Epoch: 0018 loss_train: 14.5256 acc_train: 0.3541 loss_val: 21.6190 acc_val: 0.4981 time: 0.0388s\n",
            "Epoch: 0019 loss_train: 12.7518 acc_train: 0.3214 loss_val: 18.9151 acc_val: 0.4634 time: 0.0388s\n",
            "Epoch: 0020 loss_train: 11.6100 acc_train: 0.3155 loss_val: 21.4272 acc_val: 0.4541 time: 0.0389s\n",
            "Epoch: 0021 loss_train: 10.5608 acc_train: 0.2380 loss_val: 17.3068 acc_val: 0.3965 time: 0.0388s\n",
            "Epoch: 0022 loss_train: 10.6221 acc_train: 0.2259 loss_val: 23.5181 acc_val: 0.4648 time: 0.0389s\n",
            "Epoch: 0023 loss_train: 9.6851 acc_train: 0.1988 loss_val: 21.0242 acc_val: 0.4357 time: 0.0388s\n",
            "Epoch: 0024 loss_train: 8.2985 acc_train: 0.1879 loss_val: 19.1143 acc_val: 0.4116 time: 0.0388s\n",
            "Epoch: 0025 loss_train: 7.9507 acc_train: 0.1867 loss_val: 23.0185 acc_val: 0.4543 time: 0.0390s\n",
            "Epoch: 0026 loss_train: 7.2858 acc_train: 0.1765 loss_val: 20.7864 acc_val: 0.4291 time: 0.0388s\n",
            "Epoch: 0027 loss_train: 6.2961 acc_train: 0.1744 loss_val: 21.9107 acc_val: 0.4450 time: 0.0389s\n",
            "Epoch: 0028 loss_train: 5.9351 acc_train: 0.1739 loss_val: 19.6987 acc_val: 0.4230 time: 0.0388s\n",
            "Epoch: 0029 loss_train: 6.2401 acc_train: 0.1801 loss_val: 23.7784 acc_val: 0.4606 time: 0.0389s\n",
            "Epoch: 0030 loss_train: 6.3297 acc_train: 0.1684 loss_val: 22.2601 acc_val: 0.4408 time: 0.0388s\n",
            "Epoch: 0031 loss_train: 5.8254 acc_train: 0.1646 loss_val: 20.0810 acc_val: 0.4123 time: 0.0388s\n",
            "Epoch: 0032 loss_train: 5.8772 acc_train: 0.1682 loss_val: 21.9561 acc_val: 0.4295 time: 0.0389s\n",
            "Epoch: 0033 loss_train: 5.0043 acc_train: 0.1612 loss_val: 22.0722 acc_val: 0.4262 time: 0.0388s\n",
            "Epoch: 0034 loss_train: 4.5094 acc_train: 0.1577 loss_val: 17.0003 acc_val: 0.3710 time: 0.0388s\n",
            "Epoch: 0035 loss_train: 7.5627 acc_train: 0.1786 loss_val: 23.1622 acc_val: 0.4438 time: 0.0388s\n",
            "Epoch: 0036 loss_train: 4.8803 acc_train: 0.1613 loss_val: 22.0919 acc_val: 0.4398 time: 0.0388s\n",
            "Epoch: 0037 loss_train: 4.7300 acc_train: 0.1570 loss_val: 22.1785 acc_val: 0.4444 time: 0.0387s\n",
            "Epoch: 0038 loss_train: 4.4201 acc_train: 0.1564 loss_val: 21.0535 acc_val: 0.4290 time: 0.0387s\n",
            "Epoch: 0039 loss_train: 4.2290 acc_train: 0.1520 loss_val: 21.9862 acc_val: 0.4338 time: 0.0388s\n",
            "Epoch: 0040 loss_train: 3.8523 acc_train: 0.1471 loss_val: 19.7860 acc_val: 0.4034 time: 0.0388s\n",
            "Epoch: 0041 loss_train: 4.4593 acc_train: 0.1532 loss_val: 21.6947 acc_val: 0.4261 time: 0.0388s\n",
            "Epoch: 0042 loss_train: 4.1432 acc_train: 0.1547 loss_val: 22.0353 acc_val: 0.4328 time: 0.0389s\n",
            "Epoch: 0043 loss_train: 4.0652 acc_train: 0.1543 loss_val: 21.2388 acc_val: 0.4281 time: 0.0389s\n",
            "Epoch: 0044 loss_train: 3.7672 acc_train: 0.1494 loss_val: 21.7380 acc_val: 0.4370 time: 0.0388s\n",
            "Epoch: 0045 loss_train: 3.4993 acc_train: 0.1458 loss_val: 19.5725 acc_val: 0.4128 time: 0.0389s\n",
            "Epoch: 0046 loss_train: 4.0636 acc_train: 0.1498 loss_val: 23.1935 acc_val: 0.4522 time: 0.0389s\n",
            "Epoch: 0047 loss_train: 4.0357 acc_train: 0.1485 loss_val: 22.0330 acc_val: 0.4362 time: 0.0388s\n",
            "Epoch: 0048 loss_train: 3.5949 acc_train: 0.1480 loss_val: 19.7924 acc_val: 0.4090 time: 0.0388s\n",
            "Epoch: 0049 loss_train: 4.1578 acc_train: 0.1535 loss_val: 21.4593 acc_val: 0.4265 time: 0.0388s\n",
            "Epoch: 0050 loss_train: 3.5401 acc_train: 0.1495 loss_val: 21.1429 acc_val: 0.4238 time: 0.0388s\n",
            "Epoch: 0051 loss_train: 3.3270 acc_train: 0.1470 loss_val: 21.2654 acc_val: 0.4270 time: 0.0388s\n",
            "Epoch: 0052 loss_train: 3.0665 acc_train: 0.1440 loss_val: 19.8010 acc_val: 0.4128 time: 0.0388s\n",
            "Epoch: 0053 loss_train: 3.4536 acc_train: 0.1443 loss_val: 22.6234 acc_val: 0.4470 time: 0.0388s\n",
            "Epoch: 0054 loss_train: 3.6903 acc_train: 0.1460 loss_val: 21.4962 acc_val: 0.4352 time: 0.0388s\n",
            "Epoch: 0055 loss_train: 3.3331 acc_train: 0.1466 loss_val: 19.5737 acc_val: 0.4130 time: 0.0388s\n",
            "Epoch: 0056 loss_train: 3.9494 acc_train: 0.1551 loss_val: 20.7265 acc_val: 0.4261 time: 0.0388s\n",
            "Epoch: 0057 loss_train: 3.4505 acc_train: 0.1555 loss_val: 21.2471 acc_val: 0.4310 time: 0.0389s\n",
            "Epoch: 0058 loss_train: 3.0995 acc_train: 0.1489 loss_val: 21.4156 acc_val: 0.4340 time: 0.0389s\n",
            "Epoch: 0059 loss_train: 2.8367 acc_train: 0.1445 loss_val: 20.3826 acc_val: 0.4247 time: 0.0388s\n",
            "Epoch: 0060 loss_train: 2.9553 acc_train: 0.1448 loss_val: 21.4922 acc_val: 0.4374 time: 0.0389s\n",
            "Epoch: 0061 loss_train: 3.0861 acc_train: 0.1468 loss_val: 20.6257 acc_val: 0.4276 time: 0.0389s\n",
            "Epoch: 0062 loss_train: 3.1148 acc_train: 0.1463 loss_val: 20.9564 acc_val: 0.4298 time: 0.0389s\n",
            "Epoch: 0063 loss_train: 2.8247 acc_train: 0.1453 loss_val: 20.0947 acc_val: 0.4166 time: 0.0387s\n",
            "Epoch: 0064 loss_train: 2.8480 acc_train: 0.1481 loss_val: 21.9464 acc_val: 0.4331 time: 0.0389s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 259.7278s\n",
            "Loading epoch 33\n",
            "Test set results: loss= 25.5842 accuracy= 0.5969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slPWqgtqvkft"
      },
      "source": [
        " test_output = model(test_features, adj)[0]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA0ictbZzbRu",
        "outputId": "05b5514d-7853-4c14-c1c0-15eb99bec889"
      },
      "source": [
        "test_output"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4.0534e+01, 3.2624e+01, 4.2630e+01, 2.7833e+01, 6.4298e+01, 1.3653e+01,\n",
              "        4.7058e+01, 3.7436e+01, 7.7921e+00, 1.3213e+02, 7.3364e+01, 4.2789e+01,\n",
              "        5.4028e+01, 8.6212e+01, 2.1934e+01, 2.0940e+02, 2.5173e+02, 7.3988e+01,\n",
              "        3.4791e+01, 1.0838e+02, 9.3111e+01, 2.1418e+01, 5.0501e+01, 1.4730e+02,\n",
              "        8.9587e+01, 1.6856e+02, 8.1674e+02, 4.6361e+01, 9.5481e+01, 4.7230e+01,\n",
              "        1.9868e+01, 4.2757e+01, 1.2930e+02, 1.2427e+01, 3.1829e+02, 3.8058e+01,\n",
              "        1.8617e+02, 8.7947e+01, 4.1150e+01, 2.8379e+02, 3.0611e+01, 3.3594e+01,\n",
              "        9.4564e+01, 1.9976e+02, 1.9363e+01, 1.8045e+02, 6.9916e+01, 6.0196e+01,\n",
              "        3.9365e+01, 5.3728e+01, 8.3068e+01, 4.9997e+01, 6.5588e+01, 4.3022e+01,\n",
              "        9.9396e+01, 3.8755e+01, 4.9633e+01, 1.0729e+02, 7.1818e+01, 3.7551e+01,\n",
              "        7.4857e+01, 2.1870e+01, 1.0000e+02, 3.8298e+00, 7.3914e+01, 9.0752e+01,\n",
              "        1.8219e+01, 6.5066e+01, 1.3654e+02, 1.3554e+02, 4.4853e+01, 6.2074e+01,\n",
              "        3.3595e+01, 5.6462e+01, 7.8784e+01, 1.3436e+01, 1.0080e+02, 1.3204e+01,\n",
              "        2.9539e+01, 2.3190e+01, 5.6015e+01, 1.1093e+02, 1.0823e+01, 1.9172e+02,\n",
              "        2.0978e+02, 1.4858e+01, 8.6328e+01, 2.9254e+02, 7.0524e+01, 4.1843e+01,\n",
              "        7.3080e+01, 3.0836e+01, 3.8914e+01, 1.6707e+02, 1.2009e+01, 3.0395e+01,\n",
              "        1.0837e+02, 1.4268e+02, 4.2959e+01, 2.4562e+02, 2.8526e+01, 3.9787e+01,\n",
              "        3.5097e+02, 2.9104e+01, 2.5321e+01, 2.6310e+01, 2.6710e+01, 4.8634e+01,\n",
              "        8.1086e+01, 2.2796e+01, 1.1501e+01, 2.6731e+01, 2.1407e+01, 6.3614e+01,\n",
              "        5.3998e+01, 1.3719e+02, 7.1728e+01, 6.2920e+01, 2.8073e+01, 5.4998e+01,\n",
              "        2.4180e+02, 7.3794e+01, 4.6748e+01, 5.8299e+01, 3.7737e+01, 3.2750e+01,\n",
              "        3.4971e+01, 3.6695e+01, 3.6999e+01, 6.1993e+01, 7.5999e+01, 4.9465e+01,\n",
              "        2.1850e+01, 2.1567e+01, 7.0765e+01, 3.8075e+01, 2.1332e+02, 4.6472e+01,\n",
              "        4.1675e+01, 1.0418e+02, 3.7165e+01, 2.0374e+02, 6.5564e+01, 1.4001e+01,\n",
              "        2.7606e+02, 1.1252e+02, 2.8254e+01, 1.0079e+02, 3.0875e+01, 1.5344e+02,\n",
              "        8.3021e+01, 5.0633e+01, 1.8372e+02, 2.3613e+01, 3.9907e+01, 2.7631e+01,\n",
              "        5.9351e+01, 1.9190e+02, 6.4796e+01, 4.9195e+01, 3.9305e+01, 1.5193e+01,\n",
              "        2.2946e+02, 3.7308e+01, 4.5234e+01, 2.6468e+02, 7.1511e+01, 7.7693e+01,\n",
              "        2.3393e+01, 4.9231e+01, 6.2886e+00, 8.1193e+01, 9.8856e+01, 6.9993e+01,\n",
              "        5.6020e+01, 5.2243e+01, 6.5091e+01, 8.1266e+01, 1.4517e+02, 1.9113e+02,\n",
              "        2.3611e+02, 6.0130e+01, 2.6559e+02, 2.1274e+02, 3.7976e+01, 1.8581e+02,\n",
              "        4.0708e+01, 3.7539e+01, 3.4649e+01, 3.9660e+02, 1.7636e+02, 6.9074e+01,\n",
              "        2.8172e+01, 1.9660e+01, 2.6391e+02, 6.3763e+01, 1.6598e+02, 1.4259e+01,\n",
              "        2.7609e+01, 1.7226e+01, 1.5934e+02, 4.6536e+00, 2.4058e+01, 5.1879e+01,\n",
              "        1.0142e+02, 3.5336e+01, 9.9208e+01, 1.2741e+01, 1.7993e+02, 3.3935e+01,\n",
              "        1.4955e+01, 1.0173e+01, 4.2317e+01, 4.6253e+01, 4.9187e+01, 9.3529e+01,\n",
              "        3.8773e+01, 3.2126e+02, 7.6892e+01, 8.5289e+01, 4.4436e+01, 2.2664e+00,\n",
              "        3.8844e+01, 9.6843e+00, 8.2260e+01, 3.0077e+01, 1.0085e+02, 1.8367e+01,\n",
              "        0.0000e+00, 8.7242e+01, 4.0595e+01, 3.7485e+01, 3.1113e+01, 1.4461e+02,\n",
              "        8.1297e+01, 5.7921e+01, 1.6093e+02, 5.5711e+01, 1.7708e+01, 8.3324e+01,\n",
              "        0.0000e+00, 8.3506e+01, 2.7488e+01, 6.3420e+01, 2.2906e+01, 9.5761e+01,\n",
              "        7.2027e+01, 1.4101e+02, 2.0533e+01, 7.2647e+01, 1.1553e+02, 2.5210e+02,\n",
              "        5.3712e+01, 6.5576e+01, 9.2946e+01, 5.7842e+01, 6.0753e+01, 3.3299e+01,\n",
              "        6.3233e+01, 0.0000e+00, 1.0187e+02, 3.5920e+01, 5.8425e+01, 2.8773e+01,\n",
              "        1.0087e+02, 5.1077e+01, 3.1383e+01, 3.1883e+01, 7.8278e+01, 1.3010e+01,\n",
              "        1.0391e+01, 1.1234e+02, 3.2515e+02, 2.5676e+01, 2.3503e+01, 2.3132e+01,\n",
              "        2.0355e+02, 3.2641e+01, 4.6738e+01, 1.9314e+02, 2.4751e-01, 1.3894e+02,\n",
              "        1.5149e+02, 1.2102e+02, 5.3690e+01, 2.1593e+01, 3.3639e+01, 4.0908e+01,\n",
              "        3.5372e+01, 1.0809e+02, 7.6303e+00, 1.9333e+01, 4.4883e+01, 2.6299e+01,\n",
              "        8.1634e+01, 3.4333e+01, 3.6092e+01, 3.4970e+01, 4.9477e+01, 4.4874e+01,\n",
              "        6.2253e+01, 4.2504e+01, 1.2419e+02, 1.0701e+02, 1.5088e+02, 5.8674e+01,\n",
              "        3.5937e+01, 1.6448e+02, 4.0631e+01, 3.2400e+01, 5.6561e+01, 1.8397e+01,\n",
              "        1.0179e+02, 8.7912e+01, 9.7660e+01, 3.9817e+01, 1.6261e+01, 1.3742e+02,\n",
              "        5.2123e+01, 1.4346e+02, 6.8071e+01, 6.4516e+01, 5.5658e+01, 7.8224e+01,\n",
              "        3.3602e+01, 3.9858e+01, 1.9954e+01, 7.1796e+01, 1.0191e+02, 3.7336e+01,\n",
              "        2.2155e+01, 5.2204e+01, 3.4709e+01, 1.2300e+02, 5.7712e+01, 2.9801e+01,\n",
              "        2.0852e+01, 8.2058e+00, 1.9914e+02, 7.1754e+01, 3.8984e+01, 3.7333e+01,\n",
              "        3.9439e+01, 3.2517e+01, 8.8437e+01, 2.3920e+02, 2.2353e+02, 1.7363e+02,\n",
              "        1.3018e+02, 6.8936e+01, 6.0301e+01, 4.0229e+01, 7.1103e+01, 4.5457e+01,\n",
              "        7.4567e+01, 4.2507e+01, 1.3756e+01, 1.5936e+01, 6.0168e+01, 2.4814e+01,\n",
              "        2.3321e+01, 2.1126e+01, 4.5347e+01, 1.1640e+02, 1.3949e+02, 5.6745e+01,\n",
              "        4.1832e+01, 2.2637e+02, 5.4606e+01, 6.0637e+01, 2.3534e+02, 7.9172e+01,\n",
              "        4.9698e+01, 9.9593e+01, 6.1799e+01, 8.2137e+01, 3.8696e+01, 3.1519e+01,\n",
              "        7.9631e+01], device='cuda:0', grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBtDvY-fvkfv",
        "outputId": "1374bbfd-7dc5-48e8-e5f8-72f9aa4da57a"
      },
      "source": [
        "print(stats.pearsonr(test_labels.cpu().detach().numpy(),test_output.cpu().detach().numpy()))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.9742740928769329, 4.612316351049439e-246)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUL0OSKGSeSI"
      },
      "source": [
        "## Train One Layer GAT with MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdCpu2sySFsx",
        "outputId": "2787065c-76ee-49f3-84e4-fce9067b915c"
      },
      "source": [
        "# Training settings\n",
        "nocuda = False\n",
        "fastmode = False\n",
        "sparse = False\n",
        "seed = 72\n",
        "epochs = 1000\n",
        "lr = 0.005\n",
        "weight_decay = 5e-3\n",
        "hidden = 8\n",
        "nb_heads = 8\n",
        "dropout = 0.2\n",
        "alpha = 0.2\n",
        "patience = 30\n",
        "nclass = int(1)\n",
        "\n",
        "cuda = not nocuda and torch.cuda.is_available()\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Load data\n",
        "adj, train_features, train_labels, valid_features, valid_labels, test_features, test_labels = load_data()\n",
        "\n",
        "# Model and optimizer\n",
        "model = GATMLP(nfeat=train_features.shape[1],\n",
        "                nhid=hidden,\n",
        "                nclass=nclass,\n",
        "                dropout=dropout,\n",
        "                nheads=nb_heads,\n",
        "                alpha=alpha)\n",
        "optimizer = optim.Adam(model.parameters(), \n",
        "                       lr=lr, \n",
        "                       weight_decay=weight_decay)\n",
        "print(model)\n",
        "\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    adj = adj.cuda()\n",
        "    train_features = train_features.cuda()\n",
        "    valid_features = valid_features.cuda()\n",
        "    test_features = test_features.cuda()\n",
        "    train_labels = train_labels.cuda()\n",
        "    valid_labels = valid_labels.cuda()\n",
        "    test_labels = test_labels.cuda()\n",
        "\n",
        "adj, train_features, valid_features, test_features, train_labels, valid_labels, test_labels = Variable(adj), Variable(train_features), Variable(valid_features), Variable(test_features), Variable(train_labels), Variable(valid_labels), Variable(test_labels)\n",
        "loss = nn.L1Loss()\n",
        "acc = nn.MSELoss()\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    train_output = model(train_features, adj)\n",
        "    loss_train = loss(train_output, train_labels)\n",
        "    acc_train = torch.sqrt(acc(torch.log(train_output+1), torch.log(train_labels+1)))\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        valid_output = model(valid_features, adj)\n",
        "\n",
        "        loss_val = loss(valid_output, valid_labels)\n",
        "        acc_val = torch.sqrt(acc(torch.log(valid_output+1), torch.log(valid_labels+1)))\n",
        "        print('Epoch: {:04d}'.format(epoch+1),\n",
        "              'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "              'acc_train: {:.4f}'.format(acc_train),\n",
        "              'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "              'acc_val: {:.4f}'.format(acc_val),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_val.data.item()\n",
        "\n",
        "\n",
        "def compute_test():\n",
        "    model.eval()\n",
        "    test_output = model(test_features, adj)\n",
        "    loss_test = loss(test_output, test_labels)\n",
        "    acc_test = torch.sqrt(acc(torch.log(test_output+1), torch.log(test_labels+1)))\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "loss_values = []\n",
        "bad_counter = 0\n",
        "best = epochs + 1\n",
        "best_epoch = 0\n",
        "for epoch in range(epochs):\n",
        "    loss_values.append(train(epoch))\n",
        "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "    if loss_values[-1] < best:\n",
        "        best = loss_values[-1]\n",
        "        best_epoch = epoch\n",
        "        bad_counter = 0\n",
        "    else:\n",
        "        bad_counter += 1\n",
        "\n",
        "    if bad_counter == patience:\n",
        "        break\n",
        "\n",
        "    files = glob.glob('*.pkl')\n",
        "    for file in files:\n",
        "        epoch_nb = int(file.split('.')[0])\n",
        "        if epoch_nb < best_epoch:\n",
        "            open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "            os.remove(file)\n",
        "\n",
        "files = glob.glob('*.pkl')\n",
        "for file in files:\n",
        "    epoch_nb = int(file.split('.')[0])\n",
        "    if epoch_nb > best_epoch:\n",
        "        open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "        os.remove(file)\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Restore best model;\n",
        "print('Loading epoch {}'.format(best_epoch))\n",
        "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "# Testing\n",
        "compute_test()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading covid dataset...\n",
            "GATMLP(\n",
            "  (attention_0): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_1): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_2): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_3): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_4): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_5): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_6): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_7): GraphAttentionLayer (276 -> 8)\n",
            "  (out_att): GraphAttentionLayer (64 -> 64)\n",
            "  (lin1): Linear(in_features=64, out_features=12112, bias=True)\n",
            "  (lin2): Linear(in_features=12112, out_features=6056, bias=True)\n",
            "  (lin3): Linear(in_features=6056, out_features=3028, bias=True)\n",
            "  (lin4): Linear(in_features=3028, out_features=379, bias=True)\n",
            ")\n",
            "Epoch: 0001 loss_train: 76.8587 acc_train: 3.9903 loss_val: 47.7238 acc_val: 1.6398 time: 0.0436s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:97: UserWarning: Using a target size (torch.Size([379])) that is different to the input size (torch.Size([379, 379])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([379])) that is different to the input size (torch.Size([379, 379])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0002 loss_train: 67.4299 acc_train: 1.8875 loss_val: 38.1086 acc_val: 1.0144 time: 0.0432s\n",
            "Epoch: 0003 loss_train: 55.9776 acc_train: 1.1891 loss_val: 32.3119 acc_val: 0.7689 time: 0.0435s\n",
            "Epoch: 0004 loss_train: 46.7750 acc_train: 0.8733 loss_val: 29.5350 acc_val: 0.6556 time: 0.0435s\n",
            "Epoch: 0005 loss_train: 39.7865 acc_train: 0.6781 loss_val: 28.3539 acc_val: 0.6013 time: 0.0431s\n",
            "Epoch: 0006 loss_train: 34.6863 acc_train: 0.5513 loss_val: 27.2794 acc_val: 0.5642 time: 0.0443s\n",
            "Epoch: 0007 loss_train: 30.5478 acc_train: 0.4703 loss_val: 23.1592 acc_val: 0.5020 time: 0.0432s\n",
            "Epoch: 0008 loss_train: 32.8763 acc_train: 0.5752 loss_val: 26.3831 acc_val: 0.5326 time: 0.0433s\n",
            "Epoch: 0009 loss_train: 26.3384 acc_train: 0.3846 loss_val: 25.2307 acc_val: 0.5076 time: 0.0433s\n",
            "Epoch: 0010 loss_train: 23.9279 acc_train: 0.3497 loss_val: 23.5837 acc_val: 0.4731 time: 0.0434s\n",
            "Epoch: 0011 loss_train: 21.2525 acc_train: 0.3029 loss_val: 22.7087 acc_val: 0.4608 time: 0.0434s\n",
            "Epoch: 0012 loss_train: 18.5984 acc_train: 0.2591 loss_val: 20.8085 acc_val: 0.4408 time: 0.0434s\n",
            "Epoch: 0013 loss_train: 16.4184 acc_train: 0.2390 loss_val: 18.8287 acc_val: 0.4193 time: 0.0467s\n",
            "Epoch: 0014 loss_train: 14.7855 acc_train: 0.2307 loss_val: 19.2257 acc_val: 0.4266 time: 0.0472s\n",
            "Epoch: 0015 loss_train: 12.9819 acc_train: 0.2068 loss_val: 20.1828 acc_val: 0.4360 time: 0.0431s\n",
            "Epoch: 0016 loss_train: 11.5654 acc_train: 0.1848 loss_val: 19.5190 acc_val: 0.4194 time: 0.0446s\n",
            "Epoch: 0017 loss_train: 10.5442 acc_train: 0.1749 loss_val: 21.5267 acc_val: 0.4413 time: 0.0430s\n",
            "Epoch: 0018 loss_train: 9.7846 acc_train: 0.1609 loss_val: 20.5652 acc_val: 0.4283 time: 0.0430s\n",
            "Epoch: 0019 loss_train: 9.0228 acc_train: 0.1534 loss_val: 20.2822 acc_val: 0.4213 time: 0.0448s\n",
            "Epoch: 0020 loss_train: 8.3550 acc_train: 0.1479 loss_val: 22.1989 acc_val: 0.4387 time: 0.0441s\n",
            "Epoch: 0021 loss_train: 7.8585 acc_train: 0.1390 loss_val: 19.5699 acc_val: 0.4066 time: 0.0432s\n",
            "Epoch: 0022 loss_train: 7.1409 acc_train: 0.1390 loss_val: 22.7537 acc_val: 0.4391 time: 0.0441s\n",
            "Epoch: 0023 loss_train: 6.3270 acc_train: 0.1290 loss_val: 19.5619 acc_val: 0.4019 time: 0.0441s\n",
            "Epoch: 0024 loss_train: 5.6857 acc_train: 0.1283 loss_val: 23.1184 acc_val: 0.4464 time: 0.0430s\n",
            "Epoch: 0025 loss_train: 5.4515 acc_train: 0.1246 loss_val: 22.0747 acc_val: 0.4370 time: 0.0448s\n",
            "Epoch: 0026 loss_train: 5.1068 acc_train: 0.1222 loss_val: 19.7058 acc_val: 0.4116 time: 0.0431s\n",
            "Epoch: 0027 loss_train: 5.5231 acc_train: 0.1240 loss_val: 22.2303 acc_val: 0.4372 time: 0.0436s\n",
            "Epoch: 0028 loss_train: 5.1395 acc_train: 0.1197 loss_val: 22.1125 acc_val: 0.4287 time: 0.0431s\n",
            "Epoch: 0029 loss_train: 4.9470 acc_train: 0.1178 loss_val: 21.5585 acc_val: 0.4190 time: 0.0432s\n",
            "Epoch: 0030 loss_train: 4.5726 acc_train: 0.1144 loss_val: 22.3185 acc_val: 0.4279 time: 0.0439s\n",
            "Epoch: 0031 loss_train: 4.1377 acc_train: 0.1113 loss_val: 20.7198 acc_val: 0.4087 time: 0.0433s\n",
            "Epoch: 0032 loss_train: 4.1512 acc_train: 0.1153 loss_val: 22.6724 acc_val: 0.4312 time: 0.0454s\n",
            "Epoch: 0033 loss_train: 3.9496 acc_train: 0.1115 loss_val: 22.0714 acc_val: 0.4252 time: 0.0452s\n",
            "Epoch: 0034 loss_train: 3.6929 acc_train: 0.1098 loss_val: 21.1167 acc_val: 0.4185 time: 0.0433s\n",
            "Epoch: 0035 loss_train: 3.5918 acc_train: 0.1073 loss_val: 21.2293 acc_val: 0.4252 time: 0.0436s\n",
            "Epoch: 0036 loss_train: 3.4998 acc_train: 0.1049 loss_val: 21.7771 acc_val: 0.4333 time: 0.0440s\n",
            "Epoch: 0037 loss_train: 3.5101 acc_train: 0.1055 loss_val: 20.4150 acc_val: 0.4152 time: 0.0442s\n",
            "Epoch: 0038 loss_train: 3.5348 acc_train: 0.1048 loss_val: 21.9672 acc_val: 0.4306 time: 0.0436s\n",
            "Epoch: 0039 loss_train: 3.1950 acc_train: 0.1027 loss_val: 21.0325 acc_val: 0.4178 time: 0.0440s\n",
            "Epoch: 0040 loss_train: 3.1217 acc_train: 0.1019 loss_val: 21.0211 acc_val: 0.4207 time: 0.0433s\n",
            "Epoch: 0041 loss_train: 2.9352 acc_train: 0.1012 loss_val: 21.4164 acc_val: 0.4264 time: 0.0434s\n",
            "Epoch: 0042 loss_train: 2.7574 acc_train: 0.0997 loss_val: 21.0556 acc_val: 0.4194 time: 0.0438s\n",
            "Epoch: 0043 loss_train: 2.7259 acc_train: 0.0992 loss_val: 21.7471 acc_val: 0.4276 time: 0.0449s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 162.6570s\n",
            "Loading epoch 12\n",
            "Test set results: loss= 24.5516 accuracy= 0.6251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGqAT6oxn7b2"
      },
      "source": [
        "test_output = model(test_features, adj)[0]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XQmFZbrn7b4",
        "outputId": "8fea0e01-22cd-4431-d6b9-efbf2344cfa2"
      },
      "source": [
        "print(stats.pearsonr(test_labels.cpu().detach().numpy(),test_output.cpu().detach().numpy()))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.8443125845952232, 3.4364739208252577e-104)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWSC11kiTVR1"
      },
      "source": [
        "## Train Two Layer GAT MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U2hyfB1TXPF",
        "outputId": "f9097e65-08c8-4bad-c76d-283bd6bd0fec"
      },
      "source": [
        "# Training settings\n",
        "nocuda = False\n",
        "fastmode = False\n",
        "sparse = False\n",
        "seed = 72\n",
        "epochs = 1000\n",
        "lr = 0.005\n",
        "weight_decay = 5e-3\n",
        "hidden = 8\n",
        "nb_heads = 8\n",
        "dropout = 0.2\n",
        "alpha = 0.2\n",
        "patience = 30\n",
        "nclass = int(1)\n",
        "\n",
        "cuda = not nocuda and torch.cuda.is_available()\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Load data\n",
        "adj, train_features, train_labels, valid_features, valid_labels, test_features, test_labels = load_data()\n",
        "\n",
        "# Model and optimizer\n",
        "model = TwoLayerGATMLP(nfeat=train_features.shape[1],\n",
        "                nhid=hidden,\n",
        "                nclass=nclass,\n",
        "                dropout=dropout,\n",
        "                nheads=nb_heads,\n",
        "                alpha=alpha)\n",
        "optimizer = optim.Adam(model.parameters(), \n",
        "                       lr=lr, \n",
        "                       weight_decay=weight_decay)\n",
        "print(model)\n",
        "\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    adj = adj.cuda()\n",
        "    train_features = train_features.cuda()\n",
        "    valid_features = valid_features.cuda()\n",
        "    test_features = test_features.cuda()\n",
        "    train_labels = train_labels.cuda()\n",
        "    valid_labels = valid_labels.cuda()\n",
        "    test_labels = test_labels.cuda()\n",
        "\n",
        "adj, train_features, valid_features, test_features, train_labels, valid_labels, test_labels = Variable(adj), Variable(train_features), Variable(valid_features), Variable(test_features), Variable(train_labels), Variable(valid_labels), Variable(test_labels)\n",
        "loss = nn.L1Loss()\n",
        "acc = nn.MSELoss()\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    train_output = model(train_features, adj)\n",
        "    loss_train = loss(train_output, train_labels)\n",
        "    acc_train = torch.sqrt(acc(torch.log(train_output+1), torch.log(train_labels+1)))\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        valid_output = model(valid_features, adj)\n",
        "\n",
        "        loss_val = loss(valid_output, valid_labels)\n",
        "        acc_val = torch.sqrt(acc(torch.log(valid_output+1), torch.log(valid_labels+1)))\n",
        "        print('Epoch: {:04d}'.format(epoch+1),\n",
        "              'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "              'acc_train: {:.4f}'.format(acc_train),\n",
        "              'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "              'acc_val: {:.4f}'.format(acc_val),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_val.data.item()\n",
        "\n",
        "\n",
        "def compute_test():\n",
        "    model.eval()\n",
        "    test_output = model(test_features, adj)\n",
        "    loss_test = loss(test_output, test_labels)\n",
        "    acc_test = torch.sqrt(acc(torch.log(test_output+1), torch.log(test_labels+1)))\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "loss_values = []\n",
        "bad_counter = 0\n",
        "best = epochs + 1\n",
        "best_epoch = 0\n",
        "for epoch in range(epochs):\n",
        "    loss_values.append(train(epoch))\n",
        "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "    if loss_values[-1] < best:\n",
        "        best = loss_values[-1]\n",
        "        best_epoch = epoch\n",
        "        bad_counter = 0\n",
        "    else:\n",
        "        bad_counter += 1\n",
        "\n",
        "    if bad_counter == patience:\n",
        "        break\n",
        "\n",
        "    files = glob.glob('*.pkl')\n",
        "    for file in files:\n",
        "        epoch_nb = int(file.split('.')[0])\n",
        "        if epoch_nb < best_epoch:\n",
        "            open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "            os.remove(file)\n",
        "\n",
        "files = glob.glob('*.pkl')\n",
        "for file in files:\n",
        "    epoch_nb = int(file.split('.')[0])\n",
        "    if epoch_nb > best_epoch:\n",
        "        open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "        os.remove(file)\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Restore best model;\n",
        "print('Loading epoch {}'.format(best_epoch))\n",
        "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "# Testing\n",
        "compute_test()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading covid dataset...\n",
            "TwoLayerGATMLP(\n",
            "  (attention_0): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_1): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_2): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_3): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_4): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_5): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_6): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_7): GraphAttentionLayer (276 -> 8)\n",
            "  (out_att): GraphAttentionLayer (64 -> 64)\n",
            "  (attention2_0): GraphAttentionLayer (64 -> 4)\n",
            "  (attention2_1): GraphAttentionLayer (64 -> 4)\n",
            "  (attention2_2): GraphAttentionLayer (64 -> 4)\n",
            "  (attention2_3): GraphAttentionLayer (64 -> 4)\n",
            "  (attention2_4): GraphAttentionLayer (64 -> 4)\n",
            "  (attention2_5): GraphAttentionLayer (64 -> 4)\n",
            "  (attention2_6): GraphAttentionLayer (64 -> 4)\n",
            "  (attention2_7): GraphAttentionLayer (64 -> 4)\n",
            "  (out_att2): GraphAttentionLayer (32 -> 32)\n",
            "  (lin1): Linear(in_features=12128, out_features=6056, bias=True)\n",
            "  (lin2): Linear(in_features=6056, out_features=6056, bias=True)\n",
            "  (lin3): Linear(in_features=6056, out_features=3028, bias=True)\n",
            "  (lin4): Linear(in_features=3028, out_features=379, bias=True)\n",
            ")\n",
            "Epoch: 0001 loss_train: 76.8367 acc_train: 3.9710 loss_val: 54.5064 acc_val: 3.0003 time: 0.0524s\n",
            "Epoch: 0002 loss_train: 74.3554 acc_train: 3.2791 loss_val: 50.9628 acc_val: 2.5341 time: 0.0350s\n",
            "Epoch: 0003 loss_train: 70.5045 acc_train: 2.7812 loss_val: 46.9940 acc_val: 2.1174 time: 0.0362s\n",
            "Epoch: 0004 loss_train: 66.4620 acc_train: 2.3567 loss_val: 42.6039 acc_val: 1.7066 time: 0.0400s\n",
            "Epoch: 0005 loss_train: 62.0824 acc_train: 1.9544 loss_val: 38.2126 acc_val: 1.3266 time: 0.0370s\n",
            "Epoch: 0006 loss_train: 56.3563 acc_train: 1.5424 loss_val: 35.3787 acc_val: 1.0848 time: 0.0352s\n",
            "Epoch: 0007 loss_train: 50.5925 acc_train: 1.2261 loss_val: 32.6053 acc_val: 0.9188 time: 0.0348s\n",
            "Epoch: 0008 loss_train: 46.0469 acc_train: 1.0195 loss_val: 30.5046 acc_val: 0.7590 time: 0.0402s\n",
            "Epoch: 0009 loss_train: 41.1842 acc_train: 0.7980 loss_val: 29.0282 acc_val: 0.6711 time: 0.0356s\n",
            "Epoch: 0010 loss_train: 37.3811 acc_train: 0.6688 loss_val: 27.8793 acc_val: 0.6137 time: 0.0381s\n",
            "Epoch: 0011 loss_train: 33.8778 acc_train: 0.5740 loss_val: 26.3824 acc_val: 0.5734 time: 0.0383s\n",
            "Epoch: 0012 loss_train: 30.8810 acc_train: 0.5237 loss_val: 25.9552 acc_val: 0.5582 time: 0.0348s\n",
            "Epoch: 0013 loss_train: 28.2188 acc_train: 0.4741 loss_val: 24.2934 acc_val: 0.5181 time: 0.0369s\n",
            "Epoch: 0014 loss_train: 25.8570 acc_train: 0.4353 loss_val: 23.1150 acc_val: 0.4844 time: 0.0360s\n",
            "Epoch: 0015 loss_train: 23.6111 acc_train: 0.3795 loss_val: 21.6475 acc_val: 0.4613 time: 0.0358s\n",
            "Epoch: 0016 loss_train: 21.7247 acc_train: 0.3317 loss_val: 22.1714 acc_val: 0.4746 time: 0.0380s\n",
            "Epoch: 0017 loss_train: 20.0739 acc_train: 0.3090 loss_val: 23.1202 acc_val: 0.4920 time: 0.0430s\n",
            "Epoch: 0018 loss_train: 18.2677 acc_train: 0.2845 loss_val: 19.8771 acc_val: 0.4397 time: 0.0382s\n",
            "Epoch: 0019 loss_train: 16.8700 acc_train: 0.2681 loss_val: 19.0172 acc_val: 0.4218 time: 0.0355s\n",
            "Epoch: 0020 loss_train: 15.4585 acc_train: 0.2587 loss_val: 20.5073 acc_val: 0.4351 time: 0.0362s\n",
            "Epoch: 0021 loss_train: 14.6409 acc_train: 0.2557 loss_val: 20.5268 acc_val: 0.4357 time: 0.0348s\n",
            "Epoch: 0022 loss_train: 12.9712 acc_train: 0.2292 loss_val: 20.6296 acc_val: 0.4386 time: 0.0346s\n",
            "Epoch: 0023 loss_train: 12.7625 acc_train: 0.2180 loss_val: 19.0315 acc_val: 0.4151 time: 0.0354s\n",
            "Epoch: 0024 loss_train: 12.0171 acc_train: 0.2200 loss_val: 17.9778 acc_val: 0.4019 time: 0.0355s\n",
            "Epoch: 0025 loss_train: 11.0733 acc_train: 0.2091 loss_val: 18.7686 acc_val: 0.4116 time: 0.0345s\n",
            "Epoch: 0026 loss_train: 10.2239 acc_train: 0.2087 loss_val: 22.7662 acc_val: 0.4662 time: 0.0349s\n",
            "Epoch: 0027 loss_train: 9.5021 acc_train: 0.1969 loss_val: 22.1389 acc_val: 0.4570 time: 0.0347s\n",
            "Epoch: 0028 loss_train: 9.5428 acc_train: 0.1901 loss_val: 23.0491 acc_val: 0.4644 time: 0.0360s\n",
            "Epoch: 0029 loss_train: 8.7026 acc_train: 0.1811 loss_val: 16.6309 acc_val: 0.3843 time: 0.0370s\n",
            "Epoch: 0030 loss_train: 7.9108 acc_train: 0.1778 loss_val: 17.3700 acc_val: 0.3885 time: 0.0340s\n",
            "Epoch: 0031 loss_train: 8.7846 acc_train: 0.1971 loss_val: 18.7526 acc_val: 0.4072 time: 0.0356s\n",
            "Epoch: 0032 loss_train: 8.1454 acc_train: 0.1852 loss_val: 21.0376 acc_val: 0.4452 time: 0.0350s\n",
            "Epoch: 0033 loss_train: 6.9274 acc_train: 0.1740 loss_val: 21.4287 acc_val: 0.4431 time: 0.0362s\n",
            "Epoch: 0034 loss_train: 7.7235 acc_train: 0.1828 loss_val: 19.6340 acc_val: 0.4272 time: 0.0346s\n",
            "Epoch: 0035 loss_train: 7.3989 acc_train: 0.1719 loss_val: 18.1248 acc_val: 0.4124 time: 0.0346s\n",
            "Epoch: 0036 loss_train: 6.9166 acc_train: 0.1764 loss_val: 19.2293 acc_val: 0.4132 time: 0.0384s\n",
            "Epoch: 0037 loss_train: 6.1479 acc_train: 0.1653 loss_val: 19.8596 acc_val: 0.4179 time: 0.0377s\n",
            "Epoch: 0038 loss_train: 5.6193 acc_train: 0.1600 loss_val: 20.1701 acc_val: 0.4173 time: 0.0387s\n",
            "Epoch: 0039 loss_train: 5.5008 acc_train: 0.1663 loss_val: 20.0279 acc_val: 0.4087 time: 0.0401s\n",
            "Epoch: 0040 loss_train: 5.1663 acc_train: 0.1629 loss_val: 19.3810 acc_val: 0.4069 time: 0.0409s\n",
            "Epoch: 0041 loss_train: 6.5207 acc_train: 0.1782 loss_val: 20.6977 acc_val: 0.4148 time: 0.0397s\n",
            "Epoch: 0042 loss_train: 5.4324 acc_train: 0.1667 loss_val: 20.1339 acc_val: 0.4108 time: 0.0405s\n",
            "Epoch: 0043 loss_train: 4.7744 acc_train: 0.1589 loss_val: 19.8637 acc_val: 0.4095 time: 0.0409s\n",
            "Epoch: 0044 loss_train: 4.9238 acc_train: 0.1591 loss_val: 22.8271 acc_val: 0.4460 time: 0.0396s\n",
            "Epoch: 0045 loss_train: 7.0411 acc_train: 0.1665 loss_val: 20.7294 acc_val: 0.4154 time: 0.0360s\n",
            "Epoch: 0046 loss_train: 4.5933 acc_train: 0.1669 loss_val: 19.4915 acc_val: 0.4038 time: 0.0413s\n",
            "Epoch: 0047 loss_train: 4.5588 acc_train: 0.1712 loss_val: 22.4361 acc_val: 0.4413 time: 0.0388s\n",
            "Epoch: 0048 loss_train: 4.3923 acc_train: 0.1698 loss_val: 20.0922 acc_val: 0.4118 time: 0.0399s\n",
            "Epoch: 0049 loss_train: 4.9008 acc_train: 0.1752 loss_val: 21.2174 acc_val: 0.4211 time: 0.0397s\n",
            "Epoch: 0050 loss_train: 4.7139 acc_train: 0.1713 loss_val: 18.4654 acc_val: 0.3908 time: 0.0356s\n",
            "Epoch: 0051 loss_train: 4.0256 acc_train: 0.1551 loss_val: 22.2087 acc_val: 0.4372 time: 0.0387s\n",
            "Epoch: 0052 loss_train: 4.5398 acc_train: 0.1631 loss_val: 22.0894 acc_val: 0.4370 time: 0.0437s\n",
            "Epoch: 0053 loss_train: 5.1728 acc_train: 0.1613 loss_val: 22.6829 acc_val: 0.4455 time: 0.0393s\n",
            "Epoch: 0054 loss_train: 4.8479 acc_train: 0.1635 loss_val: 22.5763 acc_val: 0.4386 time: 0.0465s\n",
            "Epoch: 0055 loss_train: 4.4952 acc_train: 0.1576 loss_val: 21.0153 acc_val: 0.4225 time: 0.0379s\n",
            "Epoch: 0056 loss_train: 3.9522 acc_train: 0.1527 loss_val: 20.5682 acc_val: 0.4205 time: 0.0375s\n",
            "Epoch: 0057 loss_train: 3.9846 acc_train: 0.1589 loss_val: 16.7287 acc_val: 0.3707 time: 0.0359s\n",
            "Epoch: 0058 loss_train: 3.7626 acc_train: 0.1569 loss_val: 18.8427 acc_val: 0.4002 time: 0.0382s\n",
            "Epoch: 0059 loss_train: 6.4500 acc_train: 0.1683 loss_val: 19.8404 acc_val: 0.4125 time: 0.0418s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 323.2259s\n",
            "Loading epoch 28\n",
            "Test set results: loss= 28.4987 accuracy= 0.6501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9lgtjGWn54y"
      },
      "source": [
        "test_output = model(test_features, adj)[0]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNfqRq1sn540",
        "outputId": "c54c8834-a481-43af-ab7b-159d0d027080"
      },
      "source": [
        "print(stats.pearsonr(test_labels.cpu().detach().numpy(),test_output.cpu().detach().numpy()))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.8792004352309963, 1.9156226314454515e-123)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNyVtvDIwafu"
      },
      "source": [
        "## Three Layer GAT MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 875
        },
        "id": "4lM1vQDWwdUD",
        "outputId": "f6314451-ee6f-416b-e1c5-7bf351266cdc"
      },
      "source": [
        "# Training settings\n",
        "nocuda = False\n",
        "fastmode = False\n",
        "sparse = False\n",
        "seed = 72\n",
        "epochs = 1000\n",
        "lr = 0.005\n",
        "weight_decay = 5e-3\n",
        "hidden = 8\n",
        "nb_heads = 8\n",
        "dropout = 0.2\n",
        "alpha = 0.2\n",
        "patience = 30\n",
        "nclass = int(1)\n",
        "\n",
        "cuda = not nocuda and torch.cuda.is_available()\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Load data\n",
        "adj, train_features, train_labels, valid_features, valid_labels, test_features, test_labels = load_data()\n",
        "\n",
        "# Model and optimizer\n",
        "model = ThreeLayerGATMLP(nfeat=train_features.shape[1],\n",
        "                nhid=hidden,\n",
        "                nclass=nclass,\n",
        "                dropout=dropout,\n",
        "                nheads=nb_heads,\n",
        "                alpha=alpha)\n",
        "optimizer = optim.Adam(model.parameters(), \n",
        "                       lr=lr, \n",
        "                       weight_decay=weight_decay)\n",
        "print(model)\n",
        "\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    adj = adj.cuda()\n",
        "    train_features = train_features.cuda()\n",
        "    valid_features = valid_features.cuda()\n",
        "    test_features = test_features.cuda()\n",
        "    train_labels = train_labels.cuda()\n",
        "    valid_labels = valid_labels.cuda()\n",
        "    test_labels = test_labels.cuda()\n",
        "\n",
        "adj, train_features, valid_features, test_features, train_labels, valid_labels, test_labels = Variable(adj), Variable(train_features), Variable(valid_features), Variable(test_features), Variable(train_labels), Variable(valid_labels), Variable(test_labels)\n",
        "loss = nn.L1Loss()\n",
        "acc = nn.MSELoss()\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    train_output = model(train_features, adj)\n",
        "    loss_train = loss(train_output, train_labels)\n",
        "    acc_train = torch.sqrt(acc(torch.log(train_output+1), torch.log(train_labels+1)))\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        valid_output = model(valid_features, adj)\n",
        "\n",
        "        loss_val = loss(valid_output, valid_labels)\n",
        "        acc_val = torch.sqrt(acc(torch.log(valid_output+1), torch.log(valid_labels+1)))\n",
        "        print('Epoch: {:04d}'.format(epoch+1),\n",
        "              'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "              'acc_train: {:.4f}'.format(acc_train),\n",
        "              'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "              'acc_val: {:.4f}'.format(acc_val),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_val.data.item()\n",
        "\n",
        "\n",
        "def compute_test():\n",
        "    model.eval()\n",
        "    test_output = model(test_features, adj)\n",
        "    loss_test = loss(test_output, test_labels)\n",
        "    acc_test = torch.sqrt(acc(torch.log(test_output+1), torch.log(test_labels+1)))\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "loss_values = []\n",
        "bad_counter = 0\n",
        "best = epochs + 1\n",
        "best_epoch = 0\n",
        "for epoch in range(epochs):\n",
        "    loss_values.append(train(epoch))\n",
        "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "    if loss_values[-1] < best:\n",
        "        best = loss_values[-1]\n",
        "        best_epoch = epoch\n",
        "        bad_counter = 0\n",
        "    else:\n",
        "        bad_counter += 1\n",
        "\n",
        "    if bad_counter == patience:\n",
        "        break\n",
        "\n",
        "    files = glob.glob('*.pkl')\n",
        "    for file in files:\n",
        "        epoch_nb = int(file.split('.')[0])\n",
        "        if epoch_nb < best_epoch:\n",
        "            open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "            os.remove(file)\n",
        "\n",
        "files = glob.glob('*.pkl')\n",
        "for file in files:\n",
        "    epoch_nb = int(file.split('.')[0])\n",
        "    if epoch_nb > best_epoch:\n",
        "        open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "        os.remove(file)\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Restore best model;\n",
        "print('Loading epoch {}'.format(best_epoch))\n",
        "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "# Testing\n",
        "compute_test()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading covid dataset...\n",
            "ThreeLayerGATMLP(\n",
            "  (attention_0): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_1): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_2): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_3): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_4): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_5): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_6): GraphAttentionLayer (276 -> 8)\n",
            "  (attention_7): GraphAttentionLayer (276 -> 8)\n",
            "  (out_att): GraphAttentionLayer (64 -> 64)\n",
            "  (attention2_0): GraphAttentionLayer (32 -> 2)\n",
            "  (attention2_1): GraphAttentionLayer (32 -> 2)\n",
            "  (attention2_2): GraphAttentionLayer (32 -> 2)\n",
            "  (attention2_3): GraphAttentionLayer (32 -> 2)\n",
            "  (attention2_4): GraphAttentionLayer (32 -> 2)\n",
            "  (attention2_5): GraphAttentionLayer (32 -> 2)\n",
            "  (attention2_6): GraphAttentionLayer (32 -> 2)\n",
            "  (attention2_7): GraphAttentionLayer (32 -> 2)\n",
            "  (out_att2): GraphAttentionLayer (32 -> 32)\n",
            "  (out_att3): GraphAttentionLayer (16 -> 16)\n",
            "  (lin1): Linear(in_features=12128, out_features=3028, bias=True)\n",
            "  (lin2): Linear(in_features=3028, out_features=3028, bias=True)\n",
            "  (lin3): Linear(in_features=3028, out_features=1518, bias=True)\n",
            "  (lin4): Linear(in_features=1518, out_features=379, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ddb56c687019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mloss_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloss_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ddb56c687019>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mtrain_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_output\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/COVID_GAT/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0matt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_att2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/COVID_GAT/models.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0matt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_att2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/COVID_GAT/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, adj)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mWh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# h.shape: (N, in_features), Wh.shape: (N, out_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_attentional_mechanism_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument mat2 in method wrapper_mm)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqkNV5NIwjRi"
      },
      "source": [
        "test_output = model(test_features, adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xRjsio7wjRk"
      },
      "source": [
        "print(stats.pearsonr(test_labels.cpu().detach().numpy(),test_output.cpu().detach().numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_kpPk5Fmyw8"
      },
      "source": [
        "## Two Layer Simple GCN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA1hUeTsmv7T"
      },
      "source": [
        "# Training settings\n",
        "nocuda = False\n",
        "fastmode = False\n",
        "sparse = False\n",
        "seed = 72\n",
        "epochs = 10000\n",
        "lr = 0.005\n",
        "weight_decay = 5e-3\n",
        "hidden = 8\n",
        "nb_heads = 8\n",
        "dropout = 0.2\n",
        "alpha = 0.2\n",
        "patience = 30\n",
        "nclass = int(1)\n",
        "\n",
        "cuda = not nocuda and torch.cuda.is_available()\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Load data\n",
        "adj, train_features, train_labels, valid_features, valid_labels, test_features, test_labels = load_data()\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=train_features.shape[1],\n",
        "                nhid=hidden,\n",
        "                nclass=nclass,\n",
        "                dropout=dropout)\n",
        "optimizer = optim.Adam(model.parameters(), \n",
        "                       lr=lr, \n",
        "                       weight_decay=weight_decay)\n",
        "print(model)\n",
        "\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "    adj = adj.cuda()\n",
        "    train_features = train_features.cuda()\n",
        "    valid_features = valid_features.cuda()\n",
        "    test_features = test_features.cuda()\n",
        "    train_labels = train_labels.cuda()\n",
        "    valid_labels = valid_labels.cuda()\n",
        "    test_labels = test_labels.cuda()\n",
        "\n",
        "adj, train_features, valid_features, test_features, train_labels, valid_labels, test_labels = Variable(adj), Variable(train_features), Variable(valid_features), Variable(test_features), Variable(train_labels), Variable(valid_labels), Variable(test_labels)\n",
        "loss = nn.L1Loss()\n",
        "acc = nn.MSELoss()\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    train_output = model(train_features, adj)\n",
        "    loss_train = loss(train_output, train_labels)\n",
        "    acc_train = torch.sqrt(acc(torch.log(train_output+1), torch.log(train_labels+1)))\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        valid_output = model(valid_features, adj)\n",
        "\n",
        "        loss_val = loss(valid_output, valid_labels)\n",
        "        acc_val = torch.sqrt(acc(torch.log(valid_output+1), torch.log(valid_labels+1)))\n",
        "        print('Epoch: {:04d}'.format(epoch+1),\n",
        "              'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "              'acc_train: {:.4f}'.format(acc_train),\n",
        "              'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "              'acc_val: {:.4f}'.format(acc_val),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_val.data.item()\n",
        "\n",
        "\n",
        "def compute_test():\n",
        "    model.eval()\n",
        "    test_output = model(test_features, adj)\n",
        "    loss_test = loss(test_output, test_labels)\n",
        "    acc_test = torch.sqrt(acc(torch.log(test_output+1), torch.log(test_labels+1)))\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "loss_values = []\n",
        "bad_counter = 0\n",
        "best = epochs + 1\n",
        "best_epoch = 0\n",
        "for epoch in range(epochs):\n",
        "    loss_values.append(train(epoch))\n",
        "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "    if loss_values[-1] < best:\n",
        "        best = loss_values[-1]\n",
        "        best_epoch = epoch\n",
        "        bad_counter = 0\n",
        "    else:\n",
        "        bad_counter += 1\n",
        "\n",
        "    if bad_counter == patience:\n",
        "        break\n",
        "\n",
        "    files = glob.glob('*.pkl')\n",
        "    for file in files:\n",
        "        epoch_nb = int(file.split('.')[0])\n",
        "        if epoch_nb < best_epoch:\n",
        "            open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "            os.remove(file)\n",
        "\n",
        "files = glob.glob('*.pkl')\n",
        "for file in files:\n",
        "    epoch_nb = int(file.split('.')[0])\n",
        "    if epoch_nb > best_epoch:\n",
        "        open(file, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n",
        "        os.remove(file)\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Restore best model;\n",
        "print('Loading epoch {}'.format(best_epoch))\n",
        "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "# Testing\n",
        "compute_test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QsxmMlzm492"
      },
      "source": [
        "test_output = model(test_features, adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTK7FYTCUWQH"
      },
      "source": [
        "print(stats.pearsonr(test_labels.cpu().detach().numpy(),test_output.cpu().detach().numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oO1J06lDnpD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}